{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0d1bdbded72f024b9331ead61a2f217ed72f340e06ce2ab6198f65373f5944641",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<bound method InteractiveShell.excepthook of <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7f1ee6950bb0>>"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "from rich import print as rprint\n",
    "from rich.traceback import install\n",
    "install()"
   ]
  },
  {
   "source": [
    "## Train a basic tokenizer from scratch\n",
    "\n",
    "ref: https://huggingface.co/docs/tokenizers/python/latest/quicktour.html#build-a-tokenizer-from-scratch"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['BPEDecoder',\n",
       " 'ByteLevel',\n",
       " 'Decoder',\n",
       " 'Metaspace',\n",
       " 'WordPiece',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " 'decoders']"
      ]
     },
     "metadata": {},
     "execution_count": 156
    }
   ],
   "source": [
    "dir(decoders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.normalizers import NFD\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tokenizers.decoders import ByteLevel as ByteLevelDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './data/wikitext-103-raw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 18min 13s, sys: 2min 19s, total: 20min 33s\nWall time: 55.2 s\n"
     ]
    }
   ],
   "source": [
    "# ---- Step 1: Tokenizer ----\n",
    "# instantiate a tokenizer with BPE model\n",
    "tokenizer = Tokenizer(BPE(unk_token='[UNK]'))\n",
    "\n",
    "# ---- Step 1.5: Normalizer ----\n",
    "tokenizer.normalizer = NFD()\n",
    "\n",
    "# ---- Step 2 & 3: Trainer and pre-tokenizer ----\n",
    "# default vocab_size is 30_000, min_frequency is 0. Define special tokens for post-process. These special tokens will be assigned id starting from 0.\n",
    "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"], vocab_size=50000)\n",
    "# The tokenizer itself might not know things like space and punctuation, i.e. it just regards the text as a sequence of characters.\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# ---- Step 4: Train ----\n",
    "# call the `train` method with a list of files.\n",
    "files = [f\"{DATA_DIR}/wiki.{split}.raw\" for split in [\"test\", \"train\", \"valid\"]]\n",
    "%time tokenizer.train(files, trainer)\n",
    "\n",
    "# ---- Step 5: Post-processor\n",
    "# add post_processor to add special tokens\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",  # `:1` for segment id\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",
    "        (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# ---- Step 6: Other configurations and saving ----\n",
    "tokenizer.decoder = BPEDecoder()  # costum decoder\n",
    "tokenizer.enable_truncation(max_length=256)\n",
    "tokenizer.enable_padding(pad_id=3, pad_token=\"[PAD]\")\n",
    "tokenizer.save(f\"{DATA_DIR}/tokenizer-wiki.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_file(f'{DATA_DIR}/tokenizer-wiki.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getnewargs__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " 'add_special_tokens',\n",
       " 'add_tokens',\n",
       " 'decode',\n",
       " 'decode_batch',\n",
       " 'decoder',\n",
       " 'enable_padding',\n",
       " 'enable_truncation',\n",
       " 'encode',\n",
       " 'encode_batch',\n",
       " 'from_buffer',\n",
       " 'from_file',\n",
       " 'from_str',\n",
       " 'get_vocab',\n",
       " 'get_vocab_size',\n",
       " 'id_to_token',\n",
       " 'model',\n",
       " 'no_padding',\n",
       " 'no_truncation',\n",
       " 'normalizer',\n",
       " 'num_special_tokens_to_add',\n",
       " 'padding',\n",
       " 'post_process',\n",
       " 'post_processor',\n",
       " 'pre_tokenizer',\n",
       " 'save',\n",
       " 'to_str',\n",
       " 'token_to_id',\n",
       " 'train',\n",
       " 'train_from_iterator',\n",
       " 'truncation']"
      ]
     },
     "metadata": {},
     "execution_count": 165
    }
   ],
   "source": [
    "dir(tokenizer)"
   ]
  },
  {
   "source": [
    "### Try to use it"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "encode a single sentence"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Encoding\u001b[1m(\u001b[0m\u001b[33mnum_tokens\u001b[0m=\u001b[1;36m14\u001b[0m, \u001b[33mattributes\u001b[0m=\u001b[1m[\u001b[0mids, type_ids, tokens, offsets, attention_mask, \nspecial_tokens_mask, overflowing\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Encoding<span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">num_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span>, <span style=\"color: #808000; text-decoration-color: #808000\">attributes</span>=<span style=\"font-weight: bold\">[</span>ids, type_ids, tokens, offsets, attention_mask, \nspecial_tokens_mask, overflowing<span style=\"font-weight: bold\">])</span>\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001b[1m[\u001b[0m\n    \u001b[32m'[CLS]'\u001b[0m,\n    \u001b[32m'Hello'\u001b[0m,\n    \u001b[32m','\u001b[0m,\n    \u001b[32m'y'\u001b[0m,\n    \u001b[32m\"'\"\u001b[0m,\n    \u001b[32m'all'\u001b[0m,\n    \u001b[32m'!'\u001b[0m,\n    \u001b[32m','\u001b[0m,\n    \u001b[32m'How'\u001b[0m,\n    \u001b[32m'are'\u001b[0m,\n    \u001b[32m'you'\u001b[0m,\n    \u001b[32m'[UNK]'\u001b[0m,\n    \u001b[32m'?'\u001b[0m,\n    \u001b[32m'[SEP]'\u001b[0m\n\u001b[1m]\u001b[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n    <span style=\"color: #008000; text-decoration-color: #008000\">'[CLS]'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'Hello'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">','</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'y'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">\"'\"</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'all'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'!'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">','</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'How'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'are'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'you'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'[UNK]'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'?'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'[SEP]'</span>\n<span style=\"font-weight: bold\">]</span>\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m26696\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m93\u001b[0m, \u001b[1;36m11\u001b[0m, \u001b[1;36m4520\u001b[0m, \u001b[1;36m5\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m7388\u001b[0m, \u001b[1;36m4535\u001b[0m, \u001b[1;36m5642\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m35\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26696</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">93</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4520</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7388</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4535</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5642</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">35</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">]</span>\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]</span>\n</pre>\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "encoding = tokenizer.encode(\"Hello, y'all!, How are you 游때 ?\")\n",
    "rprint(encoding)\n",
    "rprint(encoding.tokens)\n",
    "rprint(encoding.ids)\n",
    "rprint(encoding.type_ids)  # Is this the segment id?\n",
    "rprint(encoding.attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'游때'"
      ]
     },
     "metadata": {},
     "execution_count": 167
    }
   ],
   "source": [
    "\"Hello, y'all!, How are you 游때 ?\"[encoding.offsets[11][0]: encoding.offsets[11][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"Hello,y'all!,Howareyou?\""
      ]
     },
     "metadata": {},
     "execution_count": 170
    }
   ],
   "source": [
    "# Special tokens will be automatically removed.\n",
    "tokenizer.decode([1, 26696, 16, 93, 11, 4520, 5, 16, 7388, 4535, 5642, 0, 35, 2])  # skip_special_tokens default to True\n"
   ]
  },
  {
   "source": [
    "encode a pair of sentence"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Encoding\u001b[1m(\u001b[0m\u001b[33mnum_tokens\u001b[0m=\u001b[1;36m14\u001b[0m, \u001b[33mattributes\u001b[0m=\u001b[1m[\u001b[0mids, type_ids, tokens, offsets, attention_mask, \nspecial_tokens_mask, overflowing\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Encoding<span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">num_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span>, <span style=\"color: #808000; text-decoration-color: #808000\">attributes</span>=<span style=\"font-weight: bold\">[</span>ids, type_ids, tokens, offsets, attention_mask, \nspecial_tokens_mask, overflowing<span style=\"font-weight: bold\">])</span>\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001b[1m[\u001b[0m\n    \u001b[32m'[CLS]'\u001b[0m,\n    \u001b[32m'Hello'\u001b[0m,\n    \u001b[32m','\u001b[0m,\n    \u001b[32m'y'\u001b[0m,\n    \u001b[32m\"'\"\u001b[0m,\n    \u001b[32m'all'\u001b[0m,\n    \u001b[32m'!'\u001b[0m,\n    \u001b[32m'[SEP]'\u001b[0m,\n    \u001b[32m'How'\u001b[0m,\n    \u001b[32m'are'\u001b[0m,\n    \u001b[32m'you'\u001b[0m,\n    \u001b[32m'[UNK]'\u001b[0m,\n    \u001b[32m'?'\u001b[0m,\n    \u001b[32m'[SEP]'\u001b[0m\n\u001b[1m]\u001b[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n    <span style=\"color: #008000; text-decoration-color: #008000\">'[CLS]'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'Hello'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">','</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'y'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">\"'\"</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'all'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'!'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'[SEP]'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'How'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'are'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'you'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'[UNK]'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'?'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'[SEP]'</span>\n<span style=\"font-weight: bold\">]</span>\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m26696\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m93\u001b[0m, \u001b[1;36m11\u001b[0m, \u001b[1;36m4520\u001b[0m, \u001b[1;36m5\u001b[0m, \u001b[1;36m2\u001b[0m, \u001b[1;36m7388\u001b[0m, \u001b[1;36m4535\u001b[0m, \u001b[1;36m5642\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m35\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26696</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">93</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4520</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7388</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4535</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5642</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">35</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">]</span>\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]</span>\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]</span>\n</pre>\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "encoding = tokenizer.encode(\"Hello, y'all!\", \"How are you 游때 ?\")\n",
    "rprint(encoding)\n",
    "rprint(encoding.tokens)\n",
    "rprint(encoding.ids)\n",
    "rprint(encoding.type_ids)\n",
    "rprint(encoding.attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"Hello,y'all!Howareyou?\""
      ]
     },
     "metadata": {},
     "execution_count": 233
    }
   ],
   "source": [
    "tokenizer.decode([1, 26696, 16, 93, 11, 4520, 5, 2, 7388, 4535, 5642, 0, 35, 2])"
   ]
  },
  {
   "source": [
    "`encode_batch`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001b[1m[\u001b[0m\n    Encoding\u001b[1m(\u001b[0m\u001b[33mnum_tokens\u001b[0m=\u001b[1;36m9\u001b[0m, \u001b[33mattributes\u001b[0m=\u001b[1m[\u001b[0mids, type_ids, tokens, offsets, attention_mask, \nspecial_tokens_mask, overflowing\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n    Encoding\u001b[1m(\u001b[0m\u001b[33mnum_tokens\u001b[0m=\u001b[1;36m9\u001b[0m, \u001b[33mattributes\u001b[0m=\u001b[1m[\u001b[0mids, type_ids, tokens, offsets, attention_mask, \nspecial_tokens_mask, overflowing\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n\u001b[1m]\u001b[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n    Encoding<span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">num_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span>, <span style=\"color: #808000; text-decoration-color: #808000\">attributes</span>=<span style=\"font-weight: bold\">[</span>ids, type_ids, tokens, offsets, attention_mask, \nspecial_tokens_mask, overflowing<span style=\"font-weight: bold\">])</span>,\n    Encoding<span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">num_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span>, <span style=\"color: #808000; text-decoration-color: #808000\">attributes</span>=<span style=\"font-weight: bold\">[</span>ids, type_ids, tokens, offsets, attention_mask, \nspecial_tokens_mask, overflowing<span style=\"font-weight: bold\">])</span>\n<span style=\"font-weight: bold\">]</span>\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001b[1m[\u001b[0m\u001b[32m'[CLS]'\u001b[0m, \u001b[32m'Hello'\u001b[0m, \u001b[32m','\u001b[0m, \u001b[32m'y'\u001b[0m, \u001b[32m\"'\"\u001b[0m, \u001b[32m'all'\u001b[0m, \u001b[32m'!'\u001b[0m, \u001b[32m'[SEP]'\u001b[0m, \u001b[32m'[PAD]'\u001b[0m\u001b[1m]\u001b[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'[CLS]'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Hello'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">','</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'y'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"'\"</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'all'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'!'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'[SEP]'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'[PAD]'</span><span style=\"font-weight: bold\">]</span>\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m26696\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m93\u001b[0m, \u001b[1;36m11\u001b[0m, \u001b[1;36m4520\u001b[0m, \u001b[1;36m5\u001b[0m, \u001b[1;36m2\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m]\u001b[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26696</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">93</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4520</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">]</span>\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>\n</pre>\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "encoding = tokenizer.encode_batch([\"Hello, y'all!\", \"How are you these days 游때 ?\"])  # a flat list means a list of single sentence\n",
    "rprint(encoding)\n",
    "rprint(encoding[0].tokens)\n",
    "rprint(encoding[0].ids)\n",
    "rprint(encoding[0].type_ids)\n",
    "rprint(encoding[0].attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001b[1m[\u001b[0m\n    Encoding\u001b[1m(\u001b[0m\u001b[33mnum_tokens\u001b[0m=\u001b[1;36m16\u001b[0m, \u001b[33mattributes\u001b[0m=\u001b[1m[\u001b[0mids, type_ids, tokens, offsets, attention_mask, \nspecial_tokens_mask, overflowing\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n    Encoding\u001b[1m(\u001b[0m\u001b[33mnum_tokens\u001b[0m=\u001b[1;36m16\u001b[0m, \u001b[33mattributes\u001b[0m=\u001b[1m[\u001b[0mids, type_ids, tokens, offsets, attention_mask, \nspecial_tokens_mask, overflowing\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n\u001b[1m]\u001b[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n    Encoding<span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">num_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>, <span style=\"color: #808000; text-decoration-color: #808000\">attributes</span>=<span style=\"font-weight: bold\">[</span>ids, type_ids, tokens, offsets, attention_mask, \nspecial_tokens_mask, overflowing<span style=\"font-weight: bold\">])</span>,\n    Encoding<span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">num_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>, <span style=\"color: #808000; text-decoration-color: #808000\">attributes</span>=<span style=\"font-weight: bold\">[</span>ids, type_ids, tokens, offsets, attention_mask, \nspecial_tokens_mask, overflowing<span style=\"font-weight: bold\">])</span>\n<span style=\"font-weight: bold\">]</span>\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001b[1m[\u001b[0m\n    \u001b[32m'[CLS]'\u001b[0m,\n    \u001b[32m'Hello'\u001b[0m,\n    \u001b[32m','\u001b[0m,\n    \u001b[32m'y'\u001b[0m,\n    \u001b[32m\"'\"\u001b[0m,\n    \u001b[32m'all'\u001b[0m,\n    \u001b[32m'!'\u001b[0m,\n    \u001b[32m'[SEP]'\u001b[0m,\n    \u001b[32m'How'\u001b[0m,\n    \u001b[32m'are'\u001b[0m,\n    \u001b[32m'you'\u001b[0m,\n    \u001b[32m'[UNK]'\u001b[0m,\n    \u001b[32m'?'\u001b[0m,\n    \u001b[32m'[SEP]'\u001b[0m,\n    \u001b[32m'[PAD]'\u001b[0m,\n    \u001b[32m'[PAD]'\u001b[0m\n\u001b[1m]\u001b[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n    <span style=\"color: #008000; text-decoration-color: #008000\">'[CLS]'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'Hello'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">','</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'y'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">\"'\"</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'all'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'!'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'[SEP]'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'How'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'are'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'you'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'[UNK]'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'?'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'[SEP]'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'[PAD]'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'[PAD]'</span>\n<span style=\"font-weight: bold\">]</span>\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m26696\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m93\u001b[0m, \u001b[1;36m11\u001b[0m, \u001b[1;36m4520\u001b[0m, \u001b[1;36m5\u001b[0m, \u001b[1;36m2\u001b[0m, \u001b[1;36m7388\u001b[0m, \u001b[1;36m4535\u001b[0m, \u001b[1;36m5642\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m35\u001b[0m, \u001b[1;36m2\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m]\u001b[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26696</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">93</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4520</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7388</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4535</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5642</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">35</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">]</span>\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m26696\u001b[0m, \u001b[1;36m4453\u001b[0m, \u001b[1;36m5642\u001b[0m, \u001b[1;36m6023\u001b[0m, \u001b[1;36m5\u001b[0m, \u001b[1;36m2\u001b[0m, \u001b[1;36m45\u001b[0m, \u001b[1;36m11\u001b[0m, \u001b[1;36m81\u001b[0m, \u001b[1;36m9908\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m23360\u001b[0m, \u001b[1;36m5642\u001b[0m, \u001b[1;36m5\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26696</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4453</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5642</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6023</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">45</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">81</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9908</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">23360</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5642</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">]</span>\n</pre>\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "encoding = tokenizer.encode_batch(\n",
    "    [[\"Hello, y'all!\", \"How are you 游때 ?\"], [\"Hello to you too!\", \"I'm fine, thank you!\"]]\n",
    ")  # a list of two-element list means pair encoding\n",
    "rprint(encoding)\n",
    "rprint(encoding[0].tokens)\n",
    "rprint(encoding[0].ids)\n",
    "rprint(encoding[0].type_ids)\n",
    "rprint(encoding[0].attention_mask)\n",
    "rprint(encoding[1].ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[\"Hello , y ' all ! How are you ?\",\n",
       " \"Hello to you too ! I ' m fine , thank you !\"]"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "tokenizer.decode_batch([[1, 26696, 16, 93, 11, 4520, 5, 2, 7388, 4535, 5642, 0, 35, 2, 3, 3], [1, 26696, 4453, 5642, 6023, 5, 2, 45, 11, 81, 9908, 16, 23360, 5642, 5, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getnewargs__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " 'add_special_tokens',\n",
       " 'add_tokens',\n",
       " 'decode',\n",
       " 'decode_batch',\n",
       " 'decoder',\n",
       " 'enable_padding',\n",
       " 'enable_truncation',\n",
       " 'encode',\n",
       " 'encode_batch',\n",
       " 'from_buffer',\n",
       " 'from_file',\n",
       " 'from_str',\n",
       " 'get_vocab',\n",
       " 'get_vocab_size',\n",
       " 'id_to_token',\n",
       " 'model',\n",
       " 'no_padding',\n",
       " 'no_truncation',\n",
       " 'normalizer',\n",
       " 'num_special_tokens_to_add',\n",
       " 'padding',\n",
       " 'post_process',\n",
       " 'post_processor',\n",
       " 'pre_tokenizer',\n",
       " 'save',\n",
       " 'to_str',\n",
       " 'token_to_id',\n",
       " 'train',\n",
       " 'train_from_iterator',\n",
       " 'truncation']"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "dir(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "tokenizer.get_vocab_size()"
   ]
  },
  {
   "source": [
    "Change `post_processor`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"<CLS> $A <SEP>\",\n",
    "    pair=\"<CLS> $A <SEP> $B:1 <SEP>:1\",  # `:1` for segment id\n",
    "    special_tokens=[\n",
    "        (\"<CLS>\", tokenizer.token_to_id(\"[CLS]\")),\n",
    "        (\"<SEP>\", tokenizer.token_to_id(\"[SEP]\")),\n",
    "    ],\n",
    ")  # the special token here seems to be stand-alone ones."
   ]
  },
  {
   "source": [
    "## T5 tokenizer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5TokenizerFast\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"t5-base\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001b[1m{\u001b[0m\n    \u001b[32m'__getstate__'\u001b[0m,\n    \u001b[32m'unique_no_split_tokens'\u001b[0m,\n    \u001b[32m'__setstate__'\u001b[0m,\n    \u001b[32m'_convert_token_to_id'\u001b[0m,\n    \u001b[32m'added_tokens_decoder'\u001b[0m,\n    \u001b[32m'sp_model'\u001b[0m,\n    \u001b[32m'added_tokens_encoder'\u001b[0m,\n    \u001b[32m'prepare_for_tokenization'\u001b[0m,\n    \u001b[32m'_add_eos_if_not_present'\u001b[0m,\n    \u001b[32m'_tokenize'\u001b[0m,\n    \u001b[32m'_batch_prepare_for_model'\u001b[0m\n\u001b[1m}\u001b[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n    <span style=\"color: #008000; text-decoration-color: #008000\">'__getstate__'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'unique_no_split_tokens'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'__setstate__'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'_convert_token_to_id'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'added_tokens_decoder'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'sp_model'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'added_tokens_encoder'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'prepare_for_tokenization'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'_add_eos_if_not_present'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'_tokenize'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'_batch_prepare_for_model'</span>\n<span style=\"font-weight: bold\">}</span>\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001b[1m{\u001b[0m\n    \u001b[32m'_convert_encoding'\u001b[0m,\n    \u001b[32m'prefix_tokens'\u001b[0m,\n    \u001b[32m'backend_tokenizer'\u001b[0m,\n    \u001b[32m'_tokenizer'\u001b[0m,\n    \u001b[32m'vocab'\u001b[0m,\n    \u001b[32m'set_truncation_and_padding'\u001b[0m,\n    \u001b[32m'decoder'\u001b[0m\n\u001b[1m}\u001b[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n    <span style=\"color: #008000; text-decoration-color: #008000\">'_convert_encoding'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'prefix_tokens'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'backend_tokenizer'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'_tokenizer'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'vocab'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'set_truncation_and_padding'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'decoder'</span>\n<span style=\"font-weight: bold\">}</span>\n</pre>\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "rprint(set(dir(T5Tokenizer.from_pretrained(MODEL))) - set(dir(T5TokenizerFast.from_pretrained(MODEL))))\n",
    "rprint(set(dir(T5TokenizerFast.from_pretrained(MODEL))) - set(dir(T5Tokenizer.from_pretrained(MODEL))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['_add_tokens',\n",
       " '_additional_special_tokens',\n",
       " '_bos_token',\n",
       " '_cls_token',\n",
       " '_convert_id_to_token',\n",
       " '_eos_token',\n",
       " '_mask_token',\n",
       " '_pad_token',\n",
       " '_sep_token',\n",
       " '_unk_token',\n",
       " 'add_special_tokens',\n",
       " 'add_tokens',\n",
       " 'additional_special_tokens',\n",
       " 'all_special_tokens',\n",
       " 'bos_token',\n",
       " 'build_inputs_with_special_tokens',\n",
       " 'cls_token',\n",
       " 'convert_ids_to_tokens',\n",
       " 'eos_token',\n",
       " 'mask_token',\n",
       " 'pad_token',\n",
       " 'sanitize_special_tokens',\n",
       " 'sep_token',\n",
       " 'unique_no_split_tokens',\n",
       " 'unk_token']"
      ]
     },
     "metadata": {},
     "execution_count": 97
    }
   ],
   "source": [
    "[attr for attr in dir(tokenizer) if attr.endswith('_token') or attr.endswith('_tokens')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('~/wusuowei/data/kaggle/news_summary/news_summary_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001b[3;91mFalse\u001b[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001b[3;91mFalse\u001b[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001b[3;91mFalse\u001b[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>\n</pre>\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "rprint(hasattr(tokenizer, 'post_processor'))\n",
    "rprint(hasattr(tokenizer, 'enable_truncation'))\n",
    "rprint(hasattr(tokenizer, 'enable_padding'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "The Administration of Union Territory Daman and Diu has revoked its order that made it \ncompulsory for women to tie rakhis to their male colleagues on the occasion of Rakshabandhan \non August \u001b[1;36m7\u001b[0m. The administration was forced to withdraw the decision within \u001b[1;36m24\u001b[0m hours of \nissuing the circular after it received flak from employees and was slammed on social media.\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">The Administration of Union Territory Daman and Diu has revoked its order that made it \ncompulsory for women to tie rakhis to their male colleagues on the occasion of Rakshabandhan \non August <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>. The administration was forced to withdraw the decision within <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">24</span> hours of \nissuing the circular after it received flak from employees and was slammed on social media.\n</pre>\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "text = df.summary[0]\n",
    "rprint(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('<pad>', 0)"
      ]
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "source": [
    "tokenizer.pad_token, tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   37,  6863,    13,  3545, 30110,   878,   348,    11,  2043,    76,\n",
       "            65,     3,    52, 17943,    26,   165,   455,    24,   263,    34,\n",
       "         29701,    21,   887,    12,  6177,     3,  9782, 10193,    12,    70,\n",
       "          5069,  6976,    30,     8,  5333,    13,  2922,   157,     7,  6111,\n",
       "           232,  2618,    30,  1660,  4306,    37,  3602,    47,  5241,    12,\n",
       "         14510,     8,  1357,   441,   997,   716,    13,    19, 17180,     8,\n",
       "         15646,   227,    34,  1204,  5731,   157,    45,  1652,    11,    47,\n",
       "             3,     7,    40,   265,  2726,    30,   569,   783,     5,     1,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0]])}"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "tokenizer(\n",
    "    text,\n",
    "    max_length=100,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_attention_mask=True,\n",
    "    add_special_tokens=True,\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[   37,  6863,    13,  3545, 30110,   878,   348,    11,  2043,    76,\n",
       "            65,     3,    52, 17943,    26,   165,   455,    24,   263,    34,\n",
       "         29701,    21,   887,    12,  6177,     3,  9782, 10193,    12,    70,\n",
       "          5069,  6976,    30,     8,  5333,    13,  2922,   157,     7,  6111,\n",
       "           232,  2618,    30,  1660,  4306,    37,  3602,    47,  5241,    12,\n",
       "         14510,     8,  1357,   441,   997,   716,    13,    19, 17180,     8,\n",
       "         15646,   227,    34,  1204,  5731,   157,    45,  1652,    11,    47,\n",
       "             3,     7,    40,   265,  2726,    30,   569,   783,     5,     1,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "tokenizer.encode(\n",
    "    text,\n",
    "    max_length=100,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_attention_mask=True,\n",
    "    add_special_tokens=True,\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer.encode_plus(\n",
    "    text,\n",
    "    max_length=100,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_attention_mask=True,\n",
    "    add_special_tokens=True,\n",
    "    return_tensors=\"pt\",\n",
    ")['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'The Administration of Union Territory Daman and Diu has revoked its order that made it compulsory for women to tie rakhis to their male colleagues on the occasion of Rakshabandhan on August 7. The administration was forced to withdraw the decision within 24 hours of issuing the circular after it received flak from employees and was slammed on social media.'"
      ]
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "source": [
    "tokenizer.decode(encoding[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'<pad>'"
      ]
     },
     "metadata": {},
     "execution_count": 70
    }
   ],
   "source": [
    "tokenizer.decode(encoding[0][-1])  # input can also be a single token id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "([1,\n",
       "  2,\n",
       "  0,\n",
       "  32099,\n",
       "  32098,\n",
       "  32097,\n",
       "  32096,\n",
       "  32095,\n",
       "  32094,\n",
       "  32093,\n",
       "  32092,\n",
       "  32091,\n",
       "  32090,\n",
       "  32089,\n",
       "  32088,\n",
       "  32087,\n",
       "  32086,\n",
       "  32085,\n",
       "  32084,\n",
       "  32083,\n",
       "  32082,\n",
       "  32081,\n",
       "  32080,\n",
       "  32079,\n",
       "  32078,\n",
       "  32077,\n",
       "  32076,\n",
       "  32075,\n",
       "  32074,\n",
       "  32073,\n",
       "  32072,\n",
       "  32071,\n",
       "  32070,\n",
       "  32069,\n",
       "  32068,\n",
       "  32067,\n",
       "  32066,\n",
       "  32065,\n",
       "  32064,\n",
       "  32063,\n",
       "  32062,\n",
       "  32061,\n",
       "  32060,\n",
       "  32059,\n",
       "  32058,\n",
       "  32057,\n",
       "  32056,\n",
       "  32055,\n",
       "  32054,\n",
       "  32053,\n",
       "  32052,\n",
       "  32051,\n",
       "  32050,\n",
       "  32049,\n",
       "  32048,\n",
       "  32047,\n",
       "  32046,\n",
       "  32045,\n",
       "  32044,\n",
       "  32043,\n",
       "  32042,\n",
       "  32041,\n",
       "  32040,\n",
       "  32039,\n",
       "  32038,\n",
       "  32037,\n",
       "  32036,\n",
       "  32035,\n",
       "  32034,\n",
       "  32033,\n",
       "  32032,\n",
       "  32031,\n",
       "  32030,\n",
       "  32029,\n",
       "  32028,\n",
       "  32027,\n",
       "  32026,\n",
       "  32025,\n",
       "  32024,\n",
       "  32023,\n",
       "  32022,\n",
       "  32021,\n",
       "  32020,\n",
       "  32019,\n",
       "  32018,\n",
       "  32017,\n",
       "  32016,\n",
       "  32015,\n",
       "  32014,\n",
       "  32013,\n",
       "  32012,\n",
       "  32011,\n",
       "  32010,\n",
       "  32009,\n",
       "  32008,\n",
       "  32007,\n",
       "  32006,\n",
       "  32005,\n",
       "  32004,\n",
       "  32003,\n",
       "  32002,\n",
       "  32001,\n",
       "  32000],\n",
       " ['</s>',\n",
       "  '<unk>',\n",
       "  '<pad>',\n",
       "  '<extra_id_0>',\n",
       "  '<extra_id_1>',\n",
       "  '<extra_id_2>',\n",
       "  '<extra_id_3>',\n",
       "  '<extra_id_4>',\n",
       "  '<extra_id_5>',\n",
       "  '<extra_id_6>',\n",
       "  '<extra_id_7>',\n",
       "  '<extra_id_8>',\n",
       "  '<extra_id_9>',\n",
       "  '<extra_id_10>',\n",
       "  '<extra_id_11>',\n",
       "  '<extra_id_12>',\n",
       "  '<extra_id_13>',\n",
       "  '<extra_id_14>',\n",
       "  '<extra_id_15>',\n",
       "  '<extra_id_16>',\n",
       "  '<extra_id_17>',\n",
       "  '<extra_id_18>',\n",
       "  '<extra_id_19>',\n",
       "  '<extra_id_20>',\n",
       "  '<extra_id_21>',\n",
       "  '<extra_id_22>',\n",
       "  '<extra_id_23>',\n",
       "  '<extra_id_24>',\n",
       "  '<extra_id_25>',\n",
       "  '<extra_id_26>',\n",
       "  '<extra_id_27>',\n",
       "  '<extra_id_28>',\n",
       "  '<extra_id_29>',\n",
       "  '<extra_id_30>',\n",
       "  '<extra_id_31>',\n",
       "  '<extra_id_32>',\n",
       "  '<extra_id_33>',\n",
       "  '<extra_id_34>',\n",
       "  '<extra_id_35>',\n",
       "  '<extra_id_36>',\n",
       "  '<extra_id_37>',\n",
       "  '<extra_id_38>',\n",
       "  '<extra_id_39>',\n",
       "  '<extra_id_40>',\n",
       "  '<extra_id_41>',\n",
       "  '<extra_id_42>',\n",
       "  '<extra_id_43>',\n",
       "  '<extra_id_44>',\n",
       "  '<extra_id_45>',\n",
       "  '<extra_id_46>',\n",
       "  '<extra_id_47>',\n",
       "  '<extra_id_48>',\n",
       "  '<extra_id_49>',\n",
       "  '<extra_id_50>',\n",
       "  '<extra_id_51>',\n",
       "  '<extra_id_52>',\n",
       "  '<extra_id_53>',\n",
       "  '<extra_id_54>',\n",
       "  '<extra_id_55>',\n",
       "  '<extra_id_56>',\n",
       "  '<extra_id_57>',\n",
       "  '<extra_id_58>',\n",
       "  '<extra_id_59>',\n",
       "  '<extra_id_60>',\n",
       "  '<extra_id_61>',\n",
       "  '<extra_id_62>',\n",
       "  '<extra_id_63>',\n",
       "  '<extra_id_64>',\n",
       "  '<extra_id_65>',\n",
       "  '<extra_id_66>',\n",
       "  '<extra_id_67>',\n",
       "  '<extra_id_68>',\n",
       "  '<extra_id_69>',\n",
       "  '<extra_id_70>',\n",
       "  '<extra_id_71>',\n",
       "  '<extra_id_72>',\n",
       "  '<extra_id_73>',\n",
       "  '<extra_id_74>',\n",
       "  '<extra_id_75>',\n",
       "  '<extra_id_76>',\n",
       "  '<extra_id_77>',\n",
       "  '<extra_id_78>',\n",
       "  '<extra_id_79>',\n",
       "  '<extra_id_80>',\n",
       "  '<extra_id_81>',\n",
       "  '<extra_id_82>',\n",
       "  '<extra_id_83>',\n",
       "  '<extra_id_84>',\n",
       "  '<extra_id_85>',\n",
       "  '<extra_id_86>',\n",
       "  '<extra_id_87>',\n",
       "  '<extra_id_88>',\n",
       "  '<extra_id_89>',\n",
       "  '<extra_id_90>',\n",
       "  '<extra_id_91>',\n",
       "  '<extra_id_92>',\n",
       "  '<extra_id_93>',\n",
       "  '<extra_id_94>',\n",
       "  '<extra_id_95>',\n",
       "  '<extra_id_96>',\n",
       "  '<extra_id_97>',\n",
       "  '<extra_id_98>',\n",
       "  '<extra_id_99>'])"
      ]
     },
     "metadata": {},
     "execution_count": 72
    }
   ],
   "source": [
    "tokenizer.all_special_ids, tokenizer.all_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using mask_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "tokenizer.mask_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['郊Hello',\n",
       " ',',\n",
       " '郊',\n",
       " 'y',\n",
       " \"'\",\n",
       " 'all',\n",
       " '!',\n",
       " ',',\n",
       " '郊How',\n",
       " '郊are',\n",
       " '郊you',\n",
       " '郊these',\n",
       " '郊days',\n",
       " '郊',\n",
       " '游때',\n",
       " '郊',\n",
       " '?']"
      ]
     },
     "metadata": {},
     "execution_count": 78
    }
   ],
   "source": [
    "tokenizer.tokenize(\"Hello, y'all!, How are you these days 游때 ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"Hello, y'all!, How are you these days <unk>?</s>\""
      ]
     },
     "metadata": {},
     "execution_count": 81
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(\"Hello, y'all!, How are you these days 游때 ?\"))"
   ]
  },
  {
   "source": [
    "## Another pre-trained tokenizer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'Helsinki-NLP/opus-mt-en-ro'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "([0, 1, 59542], ['</s>', '<unk>', '<pad>'])"
      ]
     },
     "metadata": {},
     "execution_count": 88
    }
   ],
   "source": [
    "tokenizer.all_special_ids, tokenizer.all_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using mask_token, but it is not set yet.\n",
      "Using bos_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using sep_token, but it is not set yet.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(None, None, None, None)"
      ]
     },
     "metadata": {},
     "execution_count": 91
    }
   ],
   "source": [
    "tokenizer.mask_token, tokenizer.bos_token, tokenizer.cls_token, tokenizer.sep_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['_add_tokens',\n",
       " '_additional_special_tokens',\n",
       " '_bos_token',\n",
       " '_cls_token',\n",
       " '_convert_id_to_token',\n",
       " '_eos_token',\n",
       " '_mask_token',\n",
       " '_pad_token',\n",
       " '_sep_token',\n",
       " '_unk_token',\n",
       " 'add_special_tokens',\n",
       " 'add_tokens',\n",
       " 'additional_special_tokens',\n",
       " 'all_special_tokens',\n",
       " 'bos_token',\n",
       " 'build_inputs_with_special_tokens',\n",
       " 'cls_token',\n",
       " 'convert_ids_to_tokens',\n",
       " 'eos_token',\n",
       " 'mask_token',\n",
       " 'pad_token',\n",
       " 'sanitize_special_tokens',\n",
       " 'sep_token',\n",
       " 'unique_no_split_tokens',\n",
       " 'unk_token']"
      ]
     },
     "metadata": {},
     "execution_count": 94
    }
   ],
   "source": [
    "[attr for attr in dir(tokenizer) if attr.endswith('_token') or attr.endswith('_tokens')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[   68, 16472,    14,   523, 46324,   111,   506,    18,  1249,   180,\n",
       "           174, 34290,   350,   712,    30,   462,    33, 21588,    37,  1577,\n",
       "            13, 11094,    15, 15217, 15574,    13,   239,  7640, 15995,    45,\n",
       "             4, 10871,    14,  1510,  3792, 21121,  1065,  2802,    45,  3364,\n",
       "          4163,    68,  3319,    65,  7883,    13, 14556,     4,  1505,   808,\n",
       "          1164,  1351,    14, 14843,     4, 26485,   437,    33,  2548, 39356,\n",
       "           108,  7600,    18,    65,    94,  8963,  6450,    45,  1060,  3155,\n",
       "             2,     0, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542]])"
      ]
     },
     "metadata": {},
     "execution_count": 109
    }
   ],
   "source": [
    "tokenizer.encode(\n",
    "    text,\n",
    "    max_length=100,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_attention_mask=True,\n",
    "    add_special_tokens=True,\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "source": [
    "## BERT tokenizer from scratch"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.normalizers import Lowercase, NFD, Sequence, StripAccents\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tokenizers.decoders import WordPiece as WordPieceDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "tokenizer = Tokenizer(WordPiece(unk_token='[UNK]'))\n",
    "tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(WordPiece(unk_token='[UNK]'))\n",
    "tokenizer.normalizer = Sequence([NFD(), Lowercase(), StripAccents()])\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "trainer = WordPieceTrainer(\n",
    "    vocab_size=30522, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]  # why 30522??\n",
    ")\n",
    "files = [f\"./data/wikitext-103-raw/wiki.{split}.raw\" for split in [\"test\", \"train\", \"valid\"]]\n",
    "tokenizer.train(files, trainer)\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", tokenizer.token_to_id('[CLS]')),\n",
    "        (\"[SEP]\", tokenizer.token_to_id('[SEP]')),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Encoding(num_tokens=13, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "tokenizer.encode(\"Hello, y'all! How are you 游때 ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "welcome to the tok ##eni ##zer ##s library .\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">welcome to the tok ##eni ##zer ##s library .\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "hello , y ' all ! how are you ?\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">hello , y ' all ! how are you ?\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "welcome to the tokenizers library.\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">welcome to the tokenizers library.\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "hello, y'all! how are you?\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">hello, y'all! how are you?\n</pre>\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# We need a custom decoder.\n",
    "rprint(tokenizer.decode(tokenizer.encode(\"Welcome to the 游뱅 Tokenizers library.\").ids))\n",
    "rprint(tokenizer.decode(tokenizer.encode(\"Hello, y'all! How are you 游때 ?\").ids))\n",
    "tokenizer.decoder = WordPieceDecoder()\n",
    "rprint(tokenizer.decode(tokenizer.encode(\"Welcome to the 游뱅 Tokenizers library.\").ids))\n",
    "rprint(tokenizer.decode(tokenizer.encode(\"Hello, y'all! How are you 游때 ?\").ids))"
   ]
  },
  {
   "source": [
    "## Tokenizer for language modeling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ByteLevelBPETokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(None,\n",
       " <tokenizers.pre_tokenizers.ByteLevel at 0x7f7d5a7d84b0>,\n",
       " <tokenizers.processors.ByteLevel at 0x7f7d696fbcf0>,\n",
       " <tokenizers.decoders.ByteLevel at 0x7f7d696fb930>)"
      ]
     },
     "metadata": {},
     "execution_count": 282
    }
   ],
   "source": [
    "tokenizer.normalizer, tokenizer.pre_tokenizer, tokenizer.post_processor, tokenizer.decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['./data/oscar_eo/oscar.eo.txt']\n",
    "tokenizer.train(files=files, vocab_size=52_000, min_frequency=2, special_tokens=[\"<|endoftext|>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")\n",
    "tokenizer.enable_truncation(max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['./data/oscar_eo/vocab.json', './data/oscar_eo/merges.txt']"
      ]
     },
     "metadata": {},
     "execution_count": 256
    }
   ],
   "source": [
    "tokenizer.save_model('./data/oscar_eo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    \"./data/oscar_eo/vocab.json\",\n",
    "    \"./data/oscar_eo/merges.txt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tokenizers.processors.ByteLevel at 0x7f7e18552a20>"
      ]
     },
     "metadata": {},
     "execution_count": 258
    }
   ],
   "source": [
    "tokenizer.post_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Mi', '만stas', 'Juli', 'en', '.']"
      ]
     },
     "metadata": {},
     "execution_count": 235
    }
   ],
   "source": [
    "tokenizer.encode(\"Mi estas Julien.\").tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Mi, estas Julien 游때.'"
      ]
     },
     "metadata": {},
     "execution_count": 236
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(\"Mi, estas Julien 游때.\").ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-create the tokenizer in transformers\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"./data/oscar_eo/\", max_len=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Mi, estas Julien 游때.'"
      ]
     },
     "metadata": {},
     "execution_count": 244
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(\"Mi, estas Julien 游때.\"), skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LineByLineTextDataset"
   ]
  },
  {
   "source": [
    "## GPT2 tokenizer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['AlbertTokenizer',\n",
       " 'AutoTokenizer',\n",
       " 'AutoTokenizer',\n",
       " 'BartTokenizer',\n",
       " 'BarthezTokenizer',\n",
       " 'BasicTokenizer',\n",
       " 'BertGenerationTokenizer',\n",
       " 'BertJapaneseTokenizer',\n",
       " 'BertTokenizer',\n",
       " 'BertweetTokenizer',\n",
       " 'BigBirdTokenizer',\n",
       " 'BlenderbotSmallTokenizer',\n",
       " 'BlenderbotTokenizer',\n",
       " 'CTRLTokenizer',\n",
       " 'CamembertTokenizer',\n",
       " 'CharacterTokenizer',\n",
       " 'ConvBertTokenizer',\n",
       " 'DPRContextEncoderTokenizer',\n",
       " 'DPRQuestionEncoderTokenizer',\n",
       " 'DPRReaderTokenizer',\n",
       " 'DebertaTokenizer',\n",
       " 'DebertaV2Tokenizer',\n",
       " 'DistilBertTokenizer',\n",
       " 'ElectraTokenizer',\n",
       " 'FSMTTokenizer',\n",
       " 'FlaubertTokenizer',\n",
       " 'FunnelTokenizer',\n",
       " 'GPT2Tokenizer',\n",
       " 'HerbertTokenizer',\n",
       " 'LEDTokenizer',\n",
       " 'LayoutLMTokenizer',\n",
       " 'LongformerTokenizer',\n",
       " 'LxmertTokenizer',\n",
       " 'M2M100Tokenizer',\n",
       " 'MBart50Tokenizer',\n",
       " 'MBartTokenizer',\n",
       " 'MPNetTokenizer',\n",
       " 'MT5Tokenizer',\n",
       " 'MarianTokenizer',\n",
       " 'MecabTokenizer',\n",
       " 'MobileBertTokenizer',\n",
       " 'OpenAIGPTTokenizer',\n",
       " 'PegasusTokenizer',\n",
       " 'PhobertTokenizer',\n",
       " 'PreTrainedTokenizer',\n",
       " 'ProphetNetTokenizer',\n",
       " 'RagTokenizer',\n",
       " 'ReformerTokenizer',\n",
       " 'RetriBertTokenizer',\n",
       " 'RobertaTokenizer',\n",
       " 'Speech2TextTokenizer',\n",
       " 'SqueezeBertTokenizer',\n",
       " 'T5Tokenizer',\n",
       " 'T5Tokenizer',\n",
       " 'TapasTokenizer',\n",
       " 'TransfoXLTokenizer',\n",
       " 'Wav2Vec2CTCTokenizer',\n",
       " 'Wav2Vec2Tokenizer',\n",
       " 'WordpieceTokenizer',\n",
       " 'XLMProphetNetTokenizer',\n",
       " 'XLMRobertaTokenizer',\n",
       " 'XLMTokenizer',\n",
       " 'XLNetTokenizer']"
      ]
     },
     "metadata": {},
     "execution_count": 263
    }
   ],
   "source": [
    "import transformers\n",
    "[i for i in dir(transformers) if i.endswith('Tokenizer')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, GPT2Tokenizer, GPT2TokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pretrained_tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['all_special_ids', 'all_special_tokens', 'all_special_tokens_extended']"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "[i for i in dir(pretrained_tokenizer) if i.startswith('all')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[50256]"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "pretrained_tokenizer.all_special_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': [[15496, 11, 331, 6, 439, 0], [2437, 389, 345, 777, 1528, 30325, 223, 5633]], 'attention_mask': [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "pretrained_tokenizer([\"Hello, y'all!\", \"How are you these days 游때 ?\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Mi, estas Julien 游때.'"
      ]
     },
     "metadata": {},
     "execution_count": 273
    }
   ],
   "source": [
    "pretrained_tokenizer.decode(pretrained_tokenizer.encode(\"Mi, estas Julien 游때.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using custom data configuration default-2db72cd504d2a6a0\n",
      "Downloading and preparing dataset text/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/tiankang/.cache/huggingface/datasets/text/default-2db72cd504d2a6a0/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bf712a99cb4740a9b2739fd12643f7d5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "33cc9f8c55c04ca08441feea8159962d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7e8bd46a00fa4144bb6faee7cdf7c93b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset text downloaded and prepared to /home/tiankang/.cache/huggingface/datasets/text/default-2db72cd504d2a6a0/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = './data/wikitext-103-raw/'\n",
    "data_files = dict(\n",
    "    train=DATA_DIR + 'wiki.train.raw',\n",
    "    validation=DATA_DIR + 'wiki.valid.raw',\n",
    "    test=DATA_DIR + 'wiki.test.raw'\n",
    ")\n",
    "raw_datasets = load_dataset(path='text', data_files=data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "' Except for 칁sthetic Club meetings , the Tower Building remained largely unoccupied for almost fifty years and suffered significant deterioration . The 칁sthetic Club provided much @-@ needed financial support during the period and even paid the electric bill during the Great Depression . The 칁sthetic Club is still headquartered in the Tower Building . '"
      ]
     },
     "metadata": {},
     "execution_count": 302
    }
   ],
   "source": [
    "raw_datasets['train'][123]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return pretrained_tokenizer(\n",
    "        examples['text'],\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "    )\n",
    "block_size = 512\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "' Except for 칁sthetic Club meetings , the Tower Building remained largely unoccupied for almost fifty years and suffered significant deterioration . The 칁sthetic Club provided much @-@ needed financial support during the period and even paid the electric bill during the Great Depression . The 칁sthetic Club is still headquartered in the Tower Building . '"
      ]
     },
     "metadata": {},
     "execution_count": 313
    }
   ],
   "source": [
    "raw_datasets['train'][123]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': [18181, 329, 6184, 228, 301, 6587, 6289, 8292, 837, 262, 8765, 11819, 6150, 5688, 555, 28756, 329, 2048, 15334, 812, 290, 6989, 2383, 38495, 764, 383, 6184, 228, 301, 6587, 6289, 2810, 881, 2488, 12, 31, 2622, 3176, 1104, 1141, 262, 2278, 290, 772, 3432, 262, 5186, 2855, 1141, 262, 3878, 22483, 764, 383, 6184, 228, 301, 6587, 6289, 318, 991, 48583, 287, 262, 8765, 11819, 764, 220], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "metadata": {},
     "execution_count": 316
    }
   ],
   "source": [
    "tokenize_function(raw_datasets['train'][123])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=8,\n",
    "    remove_columns=['text'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    num_proc=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['attention_mask', 'input_ids', 'labels'],\n",
       "    num_rows: 228151\n",
       "})"
      ]
     },
     "metadata": {},
     "execution_count": 332
    }
   ],
   "source": [
    "lm_datasets['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Except',\n",
       " '맍or',\n",
       " '먞',\n",
       " '캳',\n",
       " 'st',\n",
       " 'hetic',\n",
       " 'Club',\n",
       " '맔eetings',\n",
       " ',',\n",
       " '맚he',\n",
       " 'Tower',\n",
       " 'Building',\n",
       " 'remained',\n",
       " '맓argely',\n",
       " '맛n',\n",
       " 'occupied',\n",
       " '맍or',\n",
       " '마lmost',\n",
       " '맍ifty',\n",
       " '맟ears',\n",
       " '마nd',\n",
       " '맙uffered',\n",
       " '맙ignificant',\n",
       " '맋eterioration',\n",
       " '.',\n",
       " 'The',\n",
       " '먞',\n",
       " '캳',\n",
       " 'st',\n",
       " 'hetic',\n",
       " 'Club',\n",
       " '맗rovided',\n",
       " '맔uch',\n",
       " '@',\n",
       " '-',\n",
       " '@',\n",
       " '맕eeded',\n",
       " '맍inancial',\n",
       " '맙upport',\n",
       " '맋uring',\n",
       " '맚he',\n",
       " '맗eriod',\n",
       " '마nd',\n",
       " '만ven',\n",
       " '맗aid',\n",
       " '맚he',\n",
       " '만lectric',\n",
       " '막ill',\n",
       " '맋uring',\n",
       " '맚he',\n",
       " 'Great',\n",
       " 'Depression',\n",
       " '.',\n",
       " 'The',\n",
       " '먞',\n",
       " '캳',\n",
       " 'st',\n",
       " 'hetic',\n",
       " 'Club',\n",
       " '말s',\n",
       " '맙till',\n",
       " '맏eadquartered',\n",
       " '말n',\n",
       " '맚he',\n",
       " 'Tower',\n",
       " 'Building',\n",
       " '.']"
      ]
     },
     "metadata": {},
     "execution_count": 334
    }
   ],
   "source": [
    "pretrained_tokenizer.tokenize('Except for 칁sthetic Club meetings , the Tower Building remained largely unoccupied for almost fifty years and suffered significant deterioration . The 칁sthetic Club provided much @-@ needed financial support during the period and even paid the electric bill during the Great Depression . The 칁sthetic Club is still headquartered in the Tower Building .')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}