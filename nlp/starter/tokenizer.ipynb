{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0d1bdbded72f024b9331ead61a2f217ed72f340e06ce2ab6198f65373f5944641",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<bound method InteractiveShell.excepthook of <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7f1ee6950bb0>>"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "from rich import print as rprint\n",
    "from rich.traceback import install\n",
    "install()"
   ]
  },
  {
   "source": [
    "## Train a basic tokenizer from scratch\n",
    "\n",
    "ref: https://huggingface.co/docs/tokenizers/python/latest/quicktour.html#build-a-tokenizer-from-scratch"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['BPEDecoder',\n",
       " 'ByteLevel',\n",
       " 'Decoder',\n",
       " 'Metaspace',\n",
       " 'WordPiece',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " 'decoders']"
      ]
     },
     "metadata": {},
     "execution_count": 156
    }
   ],
   "source": [
    "dir(decoders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.normalizers import NFD\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tokenizers.decoders import ByteLevel as ByteLevelDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './data/wikitext-103-raw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 18min 13s, sys: 2min 19s, total: 20min 33s\nWall time: 55.2 s\n"
     ]
    }
   ],
   "source": [
    "# ---- Step 1: Tokenizer ----\n",
    "# instantiate a tokenizer with BPE model\n",
    "tokenizer = Tokenizer(BPE(unk_token='[UNK]'))\n",
    "\n",
    "# ---- Step 1.5: Normalizer ----\n",
    "tokenizer.normalizer = NFD()\n",
    "\n",
    "# ---- Step 2 & 3: Trainer and pre-tokenizer ----\n",
    "# default vocab_size is 30_000, min_frequency is 0. Define special tokens for post-process. These special tokens will be assigned id starting from 0.\n",
    "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"], vocab_size=50000)\n",
    "# The tokenizer itself might not know things like space and punctuation, i.e. it just regards the text as a sequence of characters.\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# ---- Step 4: Train ----\n",
    "# call the `train` method with a list of files.\n",
    "files = [f\"{DATA_DIR}/wiki.{split}.raw\" for split in [\"test\", \"train\", \"valid\"]]\n",
    "%time tokenizer.train(files, trainer)\n",
    "\n",
    "# ---- Step 5: Post-processor\n",
    "# add post_processor to add special tokens\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",  # `:1` for segment id\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",
    "        (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# ---- Step 6: Other configurations and saving ----\n",
    "tokenizer.decoder = BPEDecoder()  # costum decoder\n",
    "tokenizer.enable_truncation(max_length=256)\n",
    "tokenizer.enable_padding(pad_id=3, pad_token=\"[PAD]\")\n",
    "tokenizer.save(f\"{DATA_DIR}/tokenizer-wiki.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_file(f'{DATA_DIR}/tokenizer-wiki.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getnewargs__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " 'add_special_tokens',\n",
       " 'add_tokens',\n",
       " 'decode',\n",
       " 'decode_batch',\n",
       " 'decoder',\n",
       " 'enable_padding',\n",
       " 'enable_truncation',\n",
       " 'encode',\n",
       " 'encode_batch',\n",
       " 'from_buffer',\n",
       " 'from_file',\n",
       " 'from_str',\n",
       " 'get_vocab',\n",
       " 'get_vocab_size',\n",
       " 'id_to_token',\n",
       " 'model',\n",
       " 'no_padding',\n",
       " 'no_truncation',\n",
       " 'normalizer',\n",
       " 'num_special_tokens_to_add',\n",
       " 'padding',\n",
       " 'post_process',\n",
       " 'post_processor',\n",
       " 'pre_tokenizer',\n",
       " 'save',\n",
       " 'to_str',\n",
       " 'token_to_id',\n",
       " 'train',\n",
       " 'train_from_iterator',\n",
       " 'truncation']"
      ]
     },
     "metadata": {},
     "execution_count": 165
    }
   ],
   "source": [
    "dir(tokenizer)"
   ]
  },
  {
   "source": [
    "### Try to use it"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "encode a single sentence"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Encoding\u001b[1m(\u001b[0m\u001b[33mnum_tokens\u001b[0m=\u001b[1;36m14\u001b[0m, \u001b[33mattributes\u001b[0m=\u001b[1m[\u001b[0mids, type_ids, tokens, offsets, attention_mask, \nspecial_tokens_mask, overflowing\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Encoding<span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">num_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span>, <span style=\"color: #808000; text-decoration-color: #808000\">attributes</span>=<span style=\"font-weight: bold\">[</span>ids, type_ids, tokens, offsets, attention_mask, \nspecial_tokens_mask, overflowing<span style=\"font-weight: bold\">])</span>\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001b[1m[\u001b[0m\n    \u001b[32m'[CLS]'\u001b[0m,\n    \u001b[32m'Hello'\u001b[0m,\n    \u001b[32m','\u001b[0m,\n    \u001b[32m'y'\u001b[0m,\n    \u001b[32m\"'\"\u001b[0m,\n    \u001b[32m'all'\u001b[0m,\n    \u001b[32m'!'\u001b[0m,\n    \u001b[32m','\u001b[0m,\n    \u001b[32m'How'\u001b[0m,\n    \u001b[32m'are'\u001b[0m,\n    \u001b[32m'you'\u001b[0m,\n    \u001b[32m'[UNK]'\u001b[0m,\n    \u001b[32m'?'\u001b[0m,\n    \u001b[32m'[SEP]'\u001b[0m\n\u001b[1m]\u001b[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n    <span style=\"color: #008000; text-decoration-color: #008000\">'[CLS]'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'Hello'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">','</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'y'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">\"'\"</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'all'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'!'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">','</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'How'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'are'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'you'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'[UNK]'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'?'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'[SEP]'</span>\n<span style=\"font-weight: bold\">]</span>\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m26696\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m93\u001b[0m, \u001b[1;36m11\u001b[0m, \u001b[1;36m4520\u001b[0m, \u001b[1;36m5\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m7388\u001b[0m, \u001b[1;36m4535\u001b[0m, \u001b[1;36m5642\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m35\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26696</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">93</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4520</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7388</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4535</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5642</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">35</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">]</span>\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]</span>\n</pre>\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "encoding = tokenizer.encode(\"Hello, y'all!, How are you 😁 ?\")\n",
    "rprint(encoding)\n",
    "rprint(encoding.tokens)\n",
    "rprint(encoding.ids)\n",
    "rprint(encoding.type_ids)  # Is this the segment id?\n",
    "rprint(encoding.attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'😁'"
      ]
     },
     "metadata": {},
     "execution_count": 167
    }
   ],
   "source": [
    "\"Hello, y'all!, How are you 😁 ?\"[encoding.offsets[11][0]: encoding.offsets[11][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"Hello,y'all!,Howareyou?\""
      ]
     },
     "metadata": {},
     "execution_count": 170
    }
   ],
   "source": [
    "# Special tokens will be automatically removed.\n",
    "tokenizer.decode([1, 26696, 16, 93, 11, 4520, 5, 16, 7388, 4535, 5642, 0, 35, 2])  # skip_special_tokens default to True\n"
   ]
  },
  {
   "source": [
    "encode a pair of sentence"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Encoding\u001b[1m(\u001b[0m\u001b[33mnum_tokens\u001b[0m=\u001b[1;36m14\u001b[0m, \u001b[33mattributes\u001b[0m=\u001b[1m[\u001b[0mids, type_ids, tokens, offsets, attention_mask, \nspecial_tokens_mask, overflowing\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Encoding<span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">num_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span>, <span style=\"color: #808000; text-decoration-color: #808000\">attributes</span>=<span style=\"font-weight: bold\">[</span>ids, type_ids, tokens, offsets, attention_mask, \nspecial_tokens_mask, overflowing<span style=\"font-weight: bold\">])</span>\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001b[1m[\u001b[0m\n    \u001b[32m'[CLS]'\u001b[0m,\n    \u001b[32m'Hello'\u001b[0m,\n    \u001b[32m','\u001b[0m,\n    \u001b[32m'y'\u001b[0m,\n    \u001b[32m\"'\"\u001b[0m,\n    \u001b[32m'all'\u001b[0m,\n    \u001b[32m'!'\u001b[0m,\n    \u001b[32m'[SEP]'\u001b[0m,\n    \u001b[32m'How'\u001b[0m,\n    \u001b[32m'are'\u001b[0m,\n    \u001b[32m'you'\u001b[0m,\n    \u001b[32m'[UNK]'\u001b[0m,\n    \u001b[32m'?'\u001b[0m,\n    \u001b[32m'[SEP]'\u001b[0m\n\u001b[1m]\u001b[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n    <span style=\"color: #008000; text-decoration-color: #008000\">'[CLS]'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'Hello'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">','</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'y'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">\"'\"</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'all'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'!'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'[SEP]'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'How'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'are'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'you'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'[UNK]'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'?'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'[SEP]'</span>\n<span style=\"font-weight: bold\">]</span>\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m26696\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m93\u001b[0m, \u001b[1;36m11\u001b[0m, \u001b[1;36m4520\u001b[0m, \u001b[1;36m5\u001b[0m, \u001b[1;36m2\u001b[0m, \u001b[1;36m7388\u001b[0m, \u001b[1;36m4535\u001b[0m, \u001b[1;36m5642\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m35\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26696</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">93</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4520</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7388</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4535</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5642</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">35</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">]</span>\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]</span>\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]</span>\n</pre>\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "encoding = tokenizer.encode(\"Hello, y'all!\", \"How are you 😁 ?\")\n",
    "rprint(encoding)\n",
    "rprint(encoding.tokens)\n",
    "rprint(encoding.ids)\n",
    "rprint(encoding.type_ids)\n",
    "rprint(encoding.attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"Hello,y'all!Howareyou?\""
      ]
     },
     "metadata": {},
     "execution_count": 233
    }
   ],
   "source": [
    "tokenizer.decode([1, 26696, 16, 93, 11, 4520, 5, 2, 7388, 4535, 5642, 0, 35, 2])"
   ]
  },
  {
   "source": [
    "`encode_batch`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001b[1m[\u001b[0m\n    Encoding\u001b[1m(\u001b[0m\u001b[33mnum_tokens\u001b[0m=\u001b[1;36m9\u001b[0m, \u001b[33mattributes\u001b[0m=\u001b[1m[\u001b[0mids, type_ids, tokens, offsets, attention_mask, \nspecial_tokens_mask, overflowing\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n    Encoding\u001b[1m(\u001b[0m\u001b[33mnum_tokens\u001b[0m=\u001b[1;36m9\u001b[0m, \u001b[33mattributes\u001b[0m=\u001b[1m[\u001b[0mids, type_ids, tokens, offsets, attention_mask, \nspecial_tokens_mask, overflowing\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n\u001b[1m]\u001b[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n    Encoding<span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">num_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span>, <span style=\"color: #808000; text-decoration-color: #808000\">attributes</span>=<span style=\"font-weight: bold\">[</span>ids, type_ids, tokens, offsets, attention_mask, \nspecial_tokens_mask, overflowing<span style=\"font-weight: bold\">])</span>,\n    Encoding<span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">num_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span>, <span style=\"color: #808000; text-decoration-color: #808000\">attributes</span>=<span style=\"font-weight: bold\">[</span>ids, type_ids, tokens, offsets, attention_mask, \nspecial_tokens_mask, overflowing<span style=\"font-weight: bold\">])</span>\n<span style=\"font-weight: bold\">]</span>\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001b[1m[\u001b[0m\u001b[32m'[CLS]'\u001b[0m, \u001b[32m'Hello'\u001b[0m, \u001b[32m','\u001b[0m, \u001b[32m'y'\u001b[0m, \u001b[32m\"'\"\u001b[0m, \u001b[32m'all'\u001b[0m, \u001b[32m'!'\u001b[0m, \u001b[32m'[SEP]'\u001b[0m, \u001b[32m'[PAD]'\u001b[0m\u001b[1m]\u001b[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'[CLS]'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Hello'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">','</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'y'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"'\"</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'all'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'!'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'[SEP]'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'[PAD]'</span><span style=\"font-weight: bold\">]</span>\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m26696\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m93\u001b[0m, \u001b[1;36m11\u001b[0m, \u001b[1;36m4520\u001b[0m, \u001b[1;36m5\u001b[0m, \u001b[1;36m2\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m]\u001b[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26696</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">93</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4520</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">]</span>\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>\n</pre>\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "encoding = tokenizer.encode_batch([\"Hello, y'all!\", \"How are you these days 😁 ?\"])  # a flat list means a list of single sentence\n",
    "rprint(encoding)\n",
    "rprint(encoding[0].tokens)\n",
    "rprint(encoding[0].ids)\n",
    "rprint(encoding[0].type_ids)\n",
    "rprint(encoding[0].attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001b[1m[\u001b[0m\n    Encoding\u001b[1m(\u001b[0m\u001b[33mnum_tokens\u001b[0m=\u001b[1;36m16\u001b[0m, \u001b[33mattributes\u001b[0m=\u001b[1m[\u001b[0mids, type_ids, tokens, offsets, attention_mask, \nspecial_tokens_mask, overflowing\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n    Encoding\u001b[1m(\u001b[0m\u001b[33mnum_tokens\u001b[0m=\u001b[1;36m16\u001b[0m, \u001b[33mattributes\u001b[0m=\u001b[1m[\u001b[0mids, type_ids, tokens, offsets, attention_mask, \nspecial_tokens_mask, overflowing\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n\u001b[1m]\u001b[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n    Encoding<span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">num_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>, <span style=\"color: #808000; text-decoration-color: #808000\">attributes</span>=<span style=\"font-weight: bold\">[</span>ids, type_ids, tokens, offsets, attention_mask, \nspecial_tokens_mask, overflowing<span style=\"font-weight: bold\">])</span>,\n    Encoding<span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">num_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>, <span style=\"color: #808000; text-decoration-color: #808000\">attributes</span>=<span style=\"font-weight: bold\">[</span>ids, type_ids, tokens, offsets, attention_mask, \nspecial_tokens_mask, overflowing<span style=\"font-weight: bold\">])</span>\n<span style=\"font-weight: bold\">]</span>\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001b[1m[\u001b[0m\n    \u001b[32m'[CLS]'\u001b[0m,\n    \u001b[32m'Hello'\u001b[0m,\n    \u001b[32m','\u001b[0m,\n    \u001b[32m'y'\u001b[0m,\n    \u001b[32m\"'\"\u001b[0m,\n    \u001b[32m'all'\u001b[0m,\n    \u001b[32m'!'\u001b[0m,\n    \u001b[32m'[SEP]'\u001b[0m,\n    \u001b[32m'How'\u001b[0m,\n    \u001b[32m'are'\u001b[0m,\n    \u001b[32m'you'\u001b[0m,\n    \u001b[32m'[UNK]'\u001b[0m,\n    \u001b[32m'?'\u001b[0m,\n    \u001b[32m'[SEP]'\u001b[0m,\n    \u001b[32m'[PAD]'\u001b[0m,\n    \u001b[32m'[PAD]'\u001b[0m\n\u001b[1m]\u001b[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n    <span style=\"color: #008000; text-decoration-color: #008000\">'[CLS]'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'Hello'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">','</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'y'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">\"'\"</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'all'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'!'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'[SEP]'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'How'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'are'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'you'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'[UNK]'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'?'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'[SEP]'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'[PAD]'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'[PAD]'</span>\n<span style=\"font-weight: bold\">]</span>\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m26696\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m93\u001b[0m, \u001b[1;36m11\u001b[0m, \u001b[1;36m4520\u001b[0m, \u001b[1;36m5\u001b[0m, \u001b[1;36m2\u001b[0m, \u001b[1;36m7388\u001b[0m, \u001b[1;36m4535\u001b[0m, \u001b[1;36m5642\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m35\u001b[0m, \u001b[1;36m2\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m]\u001b[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26696</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">93</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4520</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7388</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4535</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5642</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">35</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">]</span>\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m26696\u001b[0m, \u001b[1;36m4453\u001b[0m, \u001b[1;36m5642\u001b[0m, \u001b[1;36m6023\u001b[0m, \u001b[1;36m5\u001b[0m, \u001b[1;36m2\u001b[0m, \u001b[1;36m45\u001b[0m, \u001b[1;36m11\u001b[0m, \u001b[1;36m81\u001b[0m, \u001b[1;36m9908\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m23360\u001b[0m, \u001b[1;36m5642\u001b[0m, \u001b[1;36m5\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26696</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4453</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5642</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6023</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">45</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">81</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9908</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">23360</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5642</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">]</span>\n</pre>\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "encoding = tokenizer.encode_batch(\n",
    "    [[\"Hello, y'all!\", \"How are you 😁 ?\"], [\"Hello to you too!\", \"I'm fine, thank you!\"]]\n",
    ")  # a list of two-element list means pair encoding\n",
    "rprint(encoding)\n",
    "rprint(encoding[0].tokens)\n",
    "rprint(encoding[0].ids)\n",
    "rprint(encoding[0].type_ids)\n",
    "rprint(encoding[0].attention_mask)\n",
    "rprint(encoding[1].ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[\"Hello , y ' all ! How are you ?\",\n",
       " \"Hello to you too ! I ' m fine , thank you !\"]"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "tokenizer.decode_batch([[1, 26696, 16, 93, 11, 4520, 5, 2, 7388, 4535, 5642, 0, 35, 2, 3, 3], [1, 26696, 4453, 5642, 6023, 5, 2, 45, 11, 81, 9908, 16, 23360, 5642, 5, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getnewargs__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " 'add_special_tokens',\n",
       " 'add_tokens',\n",
       " 'decode',\n",
       " 'decode_batch',\n",
       " 'decoder',\n",
       " 'enable_padding',\n",
       " 'enable_truncation',\n",
       " 'encode',\n",
       " 'encode_batch',\n",
       " 'from_buffer',\n",
       " 'from_file',\n",
       " 'from_str',\n",
       " 'get_vocab',\n",
       " 'get_vocab_size',\n",
       " 'id_to_token',\n",
       " 'model',\n",
       " 'no_padding',\n",
       " 'no_truncation',\n",
       " 'normalizer',\n",
       " 'num_special_tokens_to_add',\n",
       " 'padding',\n",
       " 'post_process',\n",
       " 'post_processor',\n",
       " 'pre_tokenizer',\n",
       " 'save',\n",
       " 'to_str',\n",
       " 'token_to_id',\n",
       " 'train',\n",
       " 'train_from_iterator',\n",
       " 'truncation']"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "dir(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "tokenizer.get_vocab_size()"
   ]
  },
  {
   "source": [
    "Change `post_processor`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"<CLS> $A <SEP>\",\n",
    "    pair=\"<CLS> $A <SEP> $B:1 <SEP>:1\",  # `:1` for segment id\n",
    "    special_tokens=[\n",
    "        (\"<CLS>\", tokenizer.token_to_id(\"[CLS]\")),\n",
    "        (\"<SEP>\", tokenizer.token_to_id(\"[SEP]\")),\n",
    "    ],\n",
    ")  # the special token here seems to be stand-alone ones."
   ]
  },
  {
   "source": [
    "## T5 tokenizer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5TokenizerFast\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"t5-base\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001b[1m{\u001b[0m\n    \u001b[32m'__getstate__'\u001b[0m,\n    \u001b[32m'unique_no_split_tokens'\u001b[0m,\n    \u001b[32m'__setstate__'\u001b[0m,\n    \u001b[32m'_convert_token_to_id'\u001b[0m,\n    \u001b[32m'added_tokens_decoder'\u001b[0m,\n    \u001b[32m'sp_model'\u001b[0m,\n    \u001b[32m'added_tokens_encoder'\u001b[0m,\n    \u001b[32m'prepare_for_tokenization'\u001b[0m,\n    \u001b[32m'_add_eos_if_not_present'\u001b[0m,\n    \u001b[32m'_tokenize'\u001b[0m,\n    \u001b[32m'_batch_prepare_for_model'\u001b[0m\n\u001b[1m}\u001b[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n    <span style=\"color: #008000; text-decoration-color: #008000\">'__getstate__'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'unique_no_split_tokens'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'__setstate__'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'_convert_token_to_id'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'added_tokens_decoder'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'sp_model'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'added_tokens_encoder'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'prepare_for_tokenization'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'_add_eos_if_not_present'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'_tokenize'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'_batch_prepare_for_model'</span>\n<span style=\"font-weight: bold\">}</span>\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001b[1m{\u001b[0m\n    \u001b[32m'_convert_encoding'\u001b[0m,\n    \u001b[32m'prefix_tokens'\u001b[0m,\n    \u001b[32m'backend_tokenizer'\u001b[0m,\n    \u001b[32m'_tokenizer'\u001b[0m,\n    \u001b[32m'vocab'\u001b[0m,\n    \u001b[32m'set_truncation_and_padding'\u001b[0m,\n    \u001b[32m'decoder'\u001b[0m\n\u001b[1m}\u001b[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n    <span style=\"color: #008000; text-decoration-color: #008000\">'_convert_encoding'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'prefix_tokens'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'backend_tokenizer'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'_tokenizer'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'vocab'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'set_truncation_and_padding'</span>,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'decoder'</span>\n<span style=\"font-weight: bold\">}</span>\n</pre>\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "rprint(set(dir(T5Tokenizer.from_pretrained(MODEL))) - set(dir(T5TokenizerFast.from_pretrained(MODEL))))\n",
    "rprint(set(dir(T5TokenizerFast.from_pretrained(MODEL))) - set(dir(T5Tokenizer.from_pretrained(MODEL))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['_add_tokens',\n",
       " '_additional_special_tokens',\n",
       " '_bos_token',\n",
       " '_cls_token',\n",
       " '_convert_id_to_token',\n",
       " '_eos_token',\n",
       " '_mask_token',\n",
       " '_pad_token',\n",
       " '_sep_token',\n",
       " '_unk_token',\n",
       " 'add_special_tokens',\n",
       " 'add_tokens',\n",
       " 'additional_special_tokens',\n",
       " 'all_special_tokens',\n",
       " 'bos_token',\n",
       " 'build_inputs_with_special_tokens',\n",
       " 'cls_token',\n",
       " 'convert_ids_to_tokens',\n",
       " 'eos_token',\n",
       " 'mask_token',\n",
       " 'pad_token',\n",
       " 'sanitize_special_tokens',\n",
       " 'sep_token',\n",
       " 'unique_no_split_tokens',\n",
       " 'unk_token']"
      ]
     },
     "metadata": {},
     "execution_count": 97
    }
   ],
   "source": [
    "[attr for attr in dir(tokenizer) if attr.endswith('_token') or attr.endswith('_tokens')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('~/wusuowei/data/kaggle/news_summary/news_summary_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001b[3;91mFalse\u001b[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001b[3;91mFalse\u001b[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001b[3;91mFalse\u001b[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>\n</pre>\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "rprint(hasattr(tokenizer, 'post_processor'))\n",
    "rprint(hasattr(tokenizer, 'enable_truncation'))\n",
    "rprint(hasattr(tokenizer, 'enable_padding'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "The Administration of Union Territory Daman and Diu has revoked its order that made it \ncompulsory for women to tie rakhis to their male colleagues on the occasion of Rakshabandhan \non August \u001b[1;36m7\u001b[0m. The administration was forced to withdraw the decision within \u001b[1;36m24\u001b[0m hours of \nissuing the circular after it received flak from employees and was slammed on social media.\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">The Administration of Union Territory Daman and Diu has revoked its order that made it \ncompulsory for women to tie rakhis to their male colleagues on the occasion of Rakshabandhan \non August <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>. The administration was forced to withdraw the decision within <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">24</span> hours of \nissuing the circular after it received flak from employees and was slammed on social media.\n</pre>\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "text = df.summary[0]\n",
    "rprint(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('<pad>', 0)"
      ]
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "source": [
    "tokenizer.pad_token, tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   37,  6863,    13,  3545, 30110,   878,   348,    11,  2043,    76,\n",
       "            65,     3,    52, 17943,    26,   165,   455,    24,   263,    34,\n",
       "         29701,    21,   887,    12,  6177,     3,  9782, 10193,    12,    70,\n",
       "          5069,  6976,    30,     8,  5333,    13,  2922,   157,     7,  6111,\n",
       "           232,  2618,    30,  1660,  4306,    37,  3602,    47,  5241,    12,\n",
       "         14510,     8,  1357,   441,   997,   716,    13,    19, 17180,     8,\n",
       "         15646,   227,    34,  1204,  5731,   157,    45,  1652,    11,    47,\n",
       "             3,     7,    40,   265,  2726,    30,   569,   783,     5,     1,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0]])}"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "tokenizer(\n",
    "    text,\n",
    "    max_length=100,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_attention_mask=True,\n",
    "    add_special_tokens=True,\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[   37,  6863,    13,  3545, 30110,   878,   348,    11,  2043,    76,\n",
       "            65,     3,    52, 17943,    26,   165,   455,    24,   263,    34,\n",
       "         29701,    21,   887,    12,  6177,     3,  9782, 10193,    12,    70,\n",
       "          5069,  6976,    30,     8,  5333,    13,  2922,   157,     7,  6111,\n",
       "           232,  2618,    30,  1660,  4306,    37,  3602,    47,  5241,    12,\n",
       "         14510,     8,  1357,   441,   997,   716,    13,    19, 17180,     8,\n",
       "         15646,   227,    34,  1204,  5731,   157,    45,  1652,    11,    47,\n",
       "             3,     7,    40,   265,  2726,    30,   569,   783,     5,     1,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "tokenizer.encode(\n",
    "    text,\n",
    "    max_length=100,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_attention_mask=True,\n",
    "    add_special_tokens=True,\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer.encode_plus(\n",
    "    text,\n",
    "    max_length=100,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_attention_mask=True,\n",
    "    add_special_tokens=True,\n",
    "    return_tensors=\"pt\",\n",
    ")['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'The Administration of Union Territory Daman and Diu has revoked its order that made it compulsory for women to tie rakhis to their male colleagues on the occasion of Rakshabandhan on August 7. The administration was forced to withdraw the decision within 24 hours of issuing the circular after it received flak from employees and was slammed on social media.'"
      ]
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "source": [
    "tokenizer.decode(encoding[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'<pad>'"
      ]
     },
     "metadata": {},
     "execution_count": 70
    }
   ],
   "source": [
    "tokenizer.decode(encoding[0][-1])  # input can also be a single token id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "([1,\n",
       "  2,\n",
       "  0,\n",
       "  32099,\n",
       "  32098,\n",
       "  32097,\n",
       "  32096,\n",
       "  32095,\n",
       "  32094,\n",
       "  32093,\n",
       "  32092,\n",
       "  32091,\n",
       "  32090,\n",
       "  32089,\n",
       "  32088,\n",
       "  32087,\n",
       "  32086,\n",
       "  32085,\n",
       "  32084,\n",
       "  32083,\n",
       "  32082,\n",
       "  32081,\n",
       "  32080,\n",
       "  32079,\n",
       "  32078,\n",
       "  32077,\n",
       "  32076,\n",
       "  32075,\n",
       "  32074,\n",
       "  32073,\n",
       "  32072,\n",
       "  32071,\n",
       "  32070,\n",
       "  32069,\n",
       "  32068,\n",
       "  32067,\n",
       "  32066,\n",
       "  32065,\n",
       "  32064,\n",
       "  32063,\n",
       "  32062,\n",
       "  32061,\n",
       "  32060,\n",
       "  32059,\n",
       "  32058,\n",
       "  32057,\n",
       "  32056,\n",
       "  32055,\n",
       "  32054,\n",
       "  32053,\n",
       "  32052,\n",
       "  32051,\n",
       "  32050,\n",
       "  32049,\n",
       "  32048,\n",
       "  32047,\n",
       "  32046,\n",
       "  32045,\n",
       "  32044,\n",
       "  32043,\n",
       "  32042,\n",
       "  32041,\n",
       "  32040,\n",
       "  32039,\n",
       "  32038,\n",
       "  32037,\n",
       "  32036,\n",
       "  32035,\n",
       "  32034,\n",
       "  32033,\n",
       "  32032,\n",
       "  32031,\n",
       "  32030,\n",
       "  32029,\n",
       "  32028,\n",
       "  32027,\n",
       "  32026,\n",
       "  32025,\n",
       "  32024,\n",
       "  32023,\n",
       "  32022,\n",
       "  32021,\n",
       "  32020,\n",
       "  32019,\n",
       "  32018,\n",
       "  32017,\n",
       "  32016,\n",
       "  32015,\n",
       "  32014,\n",
       "  32013,\n",
       "  32012,\n",
       "  32011,\n",
       "  32010,\n",
       "  32009,\n",
       "  32008,\n",
       "  32007,\n",
       "  32006,\n",
       "  32005,\n",
       "  32004,\n",
       "  32003,\n",
       "  32002,\n",
       "  32001,\n",
       "  32000],\n",
       " ['</s>',\n",
       "  '<unk>',\n",
       "  '<pad>',\n",
       "  '<extra_id_0>',\n",
       "  '<extra_id_1>',\n",
       "  '<extra_id_2>',\n",
       "  '<extra_id_3>',\n",
       "  '<extra_id_4>',\n",
       "  '<extra_id_5>',\n",
       "  '<extra_id_6>',\n",
       "  '<extra_id_7>',\n",
       "  '<extra_id_8>',\n",
       "  '<extra_id_9>',\n",
       "  '<extra_id_10>',\n",
       "  '<extra_id_11>',\n",
       "  '<extra_id_12>',\n",
       "  '<extra_id_13>',\n",
       "  '<extra_id_14>',\n",
       "  '<extra_id_15>',\n",
       "  '<extra_id_16>',\n",
       "  '<extra_id_17>',\n",
       "  '<extra_id_18>',\n",
       "  '<extra_id_19>',\n",
       "  '<extra_id_20>',\n",
       "  '<extra_id_21>',\n",
       "  '<extra_id_22>',\n",
       "  '<extra_id_23>',\n",
       "  '<extra_id_24>',\n",
       "  '<extra_id_25>',\n",
       "  '<extra_id_26>',\n",
       "  '<extra_id_27>',\n",
       "  '<extra_id_28>',\n",
       "  '<extra_id_29>',\n",
       "  '<extra_id_30>',\n",
       "  '<extra_id_31>',\n",
       "  '<extra_id_32>',\n",
       "  '<extra_id_33>',\n",
       "  '<extra_id_34>',\n",
       "  '<extra_id_35>',\n",
       "  '<extra_id_36>',\n",
       "  '<extra_id_37>',\n",
       "  '<extra_id_38>',\n",
       "  '<extra_id_39>',\n",
       "  '<extra_id_40>',\n",
       "  '<extra_id_41>',\n",
       "  '<extra_id_42>',\n",
       "  '<extra_id_43>',\n",
       "  '<extra_id_44>',\n",
       "  '<extra_id_45>',\n",
       "  '<extra_id_46>',\n",
       "  '<extra_id_47>',\n",
       "  '<extra_id_48>',\n",
       "  '<extra_id_49>',\n",
       "  '<extra_id_50>',\n",
       "  '<extra_id_51>',\n",
       "  '<extra_id_52>',\n",
       "  '<extra_id_53>',\n",
       "  '<extra_id_54>',\n",
       "  '<extra_id_55>',\n",
       "  '<extra_id_56>',\n",
       "  '<extra_id_57>',\n",
       "  '<extra_id_58>',\n",
       "  '<extra_id_59>',\n",
       "  '<extra_id_60>',\n",
       "  '<extra_id_61>',\n",
       "  '<extra_id_62>',\n",
       "  '<extra_id_63>',\n",
       "  '<extra_id_64>',\n",
       "  '<extra_id_65>',\n",
       "  '<extra_id_66>',\n",
       "  '<extra_id_67>',\n",
       "  '<extra_id_68>',\n",
       "  '<extra_id_69>',\n",
       "  '<extra_id_70>',\n",
       "  '<extra_id_71>',\n",
       "  '<extra_id_72>',\n",
       "  '<extra_id_73>',\n",
       "  '<extra_id_74>',\n",
       "  '<extra_id_75>',\n",
       "  '<extra_id_76>',\n",
       "  '<extra_id_77>',\n",
       "  '<extra_id_78>',\n",
       "  '<extra_id_79>',\n",
       "  '<extra_id_80>',\n",
       "  '<extra_id_81>',\n",
       "  '<extra_id_82>',\n",
       "  '<extra_id_83>',\n",
       "  '<extra_id_84>',\n",
       "  '<extra_id_85>',\n",
       "  '<extra_id_86>',\n",
       "  '<extra_id_87>',\n",
       "  '<extra_id_88>',\n",
       "  '<extra_id_89>',\n",
       "  '<extra_id_90>',\n",
       "  '<extra_id_91>',\n",
       "  '<extra_id_92>',\n",
       "  '<extra_id_93>',\n",
       "  '<extra_id_94>',\n",
       "  '<extra_id_95>',\n",
       "  '<extra_id_96>',\n",
       "  '<extra_id_97>',\n",
       "  '<extra_id_98>',\n",
       "  '<extra_id_99>'])"
      ]
     },
     "metadata": {},
     "execution_count": 72
    }
   ],
   "source": [
    "tokenizer.all_special_ids, tokenizer.all_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using mask_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "tokenizer.mask_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['▁Hello',\n",
       " ',',\n",
       " '▁',\n",
       " 'y',\n",
       " \"'\",\n",
       " 'all',\n",
       " '!',\n",
       " ',',\n",
       " '▁How',\n",
       " '▁are',\n",
       " '▁you',\n",
       " '▁these',\n",
       " '▁days',\n",
       " '▁',\n",
       " '😁',\n",
       " '▁',\n",
       " '?']"
      ]
     },
     "metadata": {},
     "execution_count": 78
    }
   ],
   "source": [
    "tokenizer.tokenize(\"Hello, y'all!, How are you these days 😁 ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"Hello, y'all!, How are you these days <unk>?</s>\""
      ]
     },
     "metadata": {},
     "execution_count": 81
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(\"Hello, y'all!, How are you these days 😁 ?\"))"
   ]
  },
  {
   "source": [
    "## Another pre-trained tokenizer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'Helsinki-NLP/opus-mt-en-ro'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "([0, 1, 59542], ['</s>', '<unk>', '<pad>'])"
      ]
     },
     "metadata": {},
     "execution_count": 88
    }
   ],
   "source": [
    "tokenizer.all_special_ids, tokenizer.all_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using mask_token, but it is not set yet.\n",
      "Using bos_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using sep_token, but it is not set yet.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(None, None, None, None)"
      ]
     },
     "metadata": {},
     "execution_count": 91
    }
   ],
   "source": [
    "tokenizer.mask_token, tokenizer.bos_token, tokenizer.cls_token, tokenizer.sep_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['_add_tokens',\n",
       " '_additional_special_tokens',\n",
       " '_bos_token',\n",
       " '_cls_token',\n",
       " '_convert_id_to_token',\n",
       " '_eos_token',\n",
       " '_mask_token',\n",
       " '_pad_token',\n",
       " '_sep_token',\n",
       " '_unk_token',\n",
       " 'add_special_tokens',\n",
       " 'add_tokens',\n",
       " 'additional_special_tokens',\n",
       " 'all_special_tokens',\n",
       " 'bos_token',\n",
       " 'build_inputs_with_special_tokens',\n",
       " 'cls_token',\n",
       " 'convert_ids_to_tokens',\n",
       " 'eos_token',\n",
       " 'mask_token',\n",
       " 'pad_token',\n",
       " 'sanitize_special_tokens',\n",
       " 'sep_token',\n",
       " 'unique_no_split_tokens',\n",
       " 'unk_token']"
      ]
     },
     "metadata": {},
     "execution_count": 94
    }
   ],
   "source": [
    "[attr for attr in dir(tokenizer) if attr.endswith('_token') or attr.endswith('_tokens')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[   68, 16472,    14,   523, 46324,   111,   506,    18,  1249,   180,\n",
       "           174, 34290,   350,   712,    30,   462,    33, 21588,    37,  1577,\n",
       "            13, 11094,    15, 15217, 15574,    13,   239,  7640, 15995,    45,\n",
       "             4, 10871,    14,  1510,  3792, 21121,  1065,  2802,    45,  3364,\n",
       "          4163,    68,  3319,    65,  7883,    13, 14556,     4,  1505,   808,\n",
       "          1164,  1351,    14, 14843,     4, 26485,   437,    33,  2548, 39356,\n",
       "           108,  7600,    18,    65,    94,  8963,  6450,    45,  1060,  3155,\n",
       "             2,     0, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542]])"
      ]
     },
     "metadata": {},
     "execution_count": 109
    }
   ],
   "source": [
    "tokenizer.encode(\n",
    "    text,\n",
    "    max_length=100,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_attention_mask=True,\n",
    "    add_special_tokens=True,\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "source": [
    "## BERT tokenizer from scratch"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.normalizers import Lowercase, NFD, Sequence, StripAccents\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tokenizers.decoders import WordPiece as WordPieceDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "tokenizer = Tokenizer(WordPiece(unk_token='[UNK]'))\n",
    "tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(WordPiece(unk_token='[UNK]'))\n",
    "tokenizer.normalizer = Sequence([NFD(), Lowercase(), StripAccents()])\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "trainer = WordPieceTrainer(\n",
    "    vocab_size=30522, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]  # why 30522??\n",
    ")\n",
    "files = [f\"./data/wikitext-103-raw/wiki.{split}.raw\" for split in [\"test\", \"train\", \"valid\"]]\n",
    "tokenizer.train(files, trainer)\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", tokenizer.token_to_id('[CLS]')),\n",
    "        (\"[SEP]\", tokenizer.token_to_id('[SEP]')),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Encoding(num_tokens=13, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "tokenizer.encode(\"Hello, y'all! How are you 😁 ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "welcome to the tok ##eni ##zer ##s library .\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">welcome to the tok ##eni ##zer ##s library .\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "hello , y ' all ! how are you ?\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">hello , y ' all ! how are you ?\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "welcome to the tokenizers library.\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">welcome to the tokenizers library.\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "hello, y'all! how are you?\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">hello, y'all! how are you?\n</pre>\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# We need a custom decoder.\n",
    "rprint(tokenizer.decode(tokenizer.encode(\"Welcome to the 🤗 Tokenizers library.\").ids))\n",
    "rprint(tokenizer.decode(tokenizer.encode(\"Hello, y'all! How are you 😁 ?\").ids))\n",
    "tokenizer.decoder = WordPieceDecoder()\n",
    "rprint(tokenizer.decode(tokenizer.encode(\"Welcome to the 🤗 Tokenizers library.\").ids))\n",
    "rprint(tokenizer.decode(tokenizer.encode(\"Hello, y'all! How are you 😁 ?\").ids))"
   ]
  },
  {
   "source": [
    "## Tokenizer for language modeling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ByteLevelBPETokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(None,\n",
       " <tokenizers.pre_tokenizers.ByteLevel at 0x7f7d5a7d84b0>,\n",
       " <tokenizers.processors.ByteLevel at 0x7f7d696fbcf0>,\n",
       " <tokenizers.decoders.ByteLevel at 0x7f7d696fb930>)"
      ]
     },
     "metadata": {},
     "execution_count": 282
    }
   ],
   "source": [
    "tokenizer.normalizer, tokenizer.pre_tokenizer, tokenizer.post_processor, tokenizer.decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['./data/oscar_eo/oscar.eo.txt']\n",
    "tokenizer.train(files=files, vocab_size=52_000, min_frequency=2, special_tokens=[\"<|endoftext|>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")\n",
    "tokenizer.enable_truncation(max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['./data/oscar_eo/vocab.json', './data/oscar_eo/merges.txt']"
      ]
     },
     "metadata": {},
     "execution_count": 256
    }
   ],
   "source": [
    "tokenizer.save_model('./data/oscar_eo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    \"./data/oscar_eo/vocab.json\",\n",
    "    \"./data/oscar_eo/merges.txt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tokenizers.processors.ByteLevel at 0x7f7e18552a20>"
      ]
     },
     "metadata": {},
     "execution_count": 258
    }
   ],
   "source": [
    "tokenizer.post_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Mi', 'Ġestas', 'ĠJuli', 'en', '.']"
      ]
     },
     "metadata": {},
     "execution_count": 235
    }
   ],
   "source": [
    "tokenizer.encode(\"Mi estas Julien.\").tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Mi, estas Julien 😁.'"
      ]
     },
     "metadata": {},
     "execution_count": 236
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(\"Mi, estas Julien 😁.\").ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-create the tokenizer in transformers\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"./data/oscar_eo/\", max_len=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Mi, estas Julien 😁.'"
      ]
     },
     "metadata": {},
     "execution_count": 244
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(\"Mi, estas Julien 😁.\"), skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LineByLineTextDataset"
   ]
  },
  {
   "source": [
    "## GPT2 tokenizer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['AlbertTokenizer',\n",
       " 'AutoTokenizer',\n",
       " 'AutoTokenizer',\n",
       " 'BartTokenizer',\n",
       " 'BarthezTokenizer',\n",
       " 'BasicTokenizer',\n",
       " 'BertGenerationTokenizer',\n",
       " 'BertJapaneseTokenizer',\n",
       " 'BertTokenizer',\n",
       " 'BertweetTokenizer',\n",
       " 'BigBirdTokenizer',\n",
       " 'BlenderbotSmallTokenizer',\n",
       " 'BlenderbotTokenizer',\n",
       " 'CTRLTokenizer',\n",
       " 'CamembertTokenizer',\n",
       " 'CharacterTokenizer',\n",
       " 'ConvBertTokenizer',\n",
       " 'DPRContextEncoderTokenizer',\n",
       " 'DPRQuestionEncoderTokenizer',\n",
       " 'DPRReaderTokenizer',\n",
       " 'DebertaTokenizer',\n",
       " 'DebertaV2Tokenizer',\n",
       " 'DistilBertTokenizer',\n",
       " 'ElectraTokenizer',\n",
       " 'FSMTTokenizer',\n",
       " 'FlaubertTokenizer',\n",
       " 'FunnelTokenizer',\n",
       " 'GPT2Tokenizer',\n",
       " 'HerbertTokenizer',\n",
       " 'LEDTokenizer',\n",
       " 'LayoutLMTokenizer',\n",
       " 'LongformerTokenizer',\n",
       " 'LxmertTokenizer',\n",
       " 'M2M100Tokenizer',\n",
       " 'MBart50Tokenizer',\n",
       " 'MBartTokenizer',\n",
       " 'MPNetTokenizer',\n",
       " 'MT5Tokenizer',\n",
       " 'MarianTokenizer',\n",
       " 'MecabTokenizer',\n",
       " 'MobileBertTokenizer',\n",
       " 'OpenAIGPTTokenizer',\n",
       " 'PegasusTokenizer',\n",
       " 'PhobertTokenizer',\n",
       " 'PreTrainedTokenizer',\n",
       " 'ProphetNetTokenizer',\n",
       " 'RagTokenizer',\n",
       " 'ReformerTokenizer',\n",
       " 'RetriBertTokenizer',\n",
       " 'RobertaTokenizer',\n",
       " 'Speech2TextTokenizer',\n",
       " 'SqueezeBertTokenizer',\n",
       " 'T5Tokenizer',\n",
       " 'T5Tokenizer',\n",
       " 'TapasTokenizer',\n",
       " 'TransfoXLTokenizer',\n",
       " 'Wav2Vec2CTCTokenizer',\n",
       " 'Wav2Vec2Tokenizer',\n",
       " 'WordpieceTokenizer',\n",
       " 'XLMProphetNetTokenizer',\n",
       " 'XLMRobertaTokenizer',\n",
       " 'XLMTokenizer',\n",
       " 'XLNetTokenizer']"
      ]
     },
     "metadata": {},
     "execution_count": 263
    }
   ],
   "source": [
    "import transformers\n",
    "[i for i in dir(transformers) if i.endswith('Tokenizer')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, GPT2Tokenizer, GPT2TokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pretrained_tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['all_special_ids', 'all_special_tokens', 'all_special_tokens_extended']"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "[i for i in dir(pretrained_tokenizer) if i.startswith('all')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[50256]"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "pretrained_tokenizer.all_special_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': [[15496, 11, 331, 6, 439, 0], [2437, 389, 345, 777, 1528, 30325, 223, 5633]], 'attention_mask': [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "pretrained_tokenizer([\"Hello, y'all!\", \"How are you these days 😁 ?\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Mi, estas Julien 😁.'"
      ]
     },
     "metadata": {},
     "execution_count": 273
    }
   ],
   "source": [
    "pretrained_tokenizer.decode(pretrained_tokenizer.encode(\"Mi, estas Julien 😁.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using custom data configuration default-2db72cd504d2a6a0\n",
      "Downloading and preparing dataset text/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/tiankang/.cache/huggingface/datasets/text/default-2db72cd504d2a6a0/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bf712a99cb4740a9b2739fd12643f7d5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "33cc9f8c55c04ca08441feea8159962d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7e8bd46a00fa4144bb6faee7cdf7c93b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset text downloaded and prepared to /home/tiankang/.cache/huggingface/datasets/text/default-2db72cd504d2a6a0/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = './data/wikitext-103-raw/'\n",
    "data_files = dict(\n",
    "    train=DATA_DIR + 'wiki.train.raw',\n",
    "    validation=DATA_DIR + 'wiki.valid.raw',\n",
    "    test=DATA_DIR + 'wiki.test.raw'\n",
    ")\n",
    "raw_datasets = load_dataset(path='text', data_files=data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "' Except for Æsthetic Club meetings , the Tower Building remained largely unoccupied for almost fifty years and suffered significant deterioration . The Æsthetic Club provided much @-@ needed financial support during the period and even paid the electric bill during the Great Depression . The Æsthetic Club is still headquartered in the Tower Building . '"
      ]
     },
     "metadata": {},
     "execution_count": 302
    }
   ],
   "source": [
    "raw_datasets['train'][123]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return pretrained_tokenizer(\n",
    "        examples['text'],\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "    )\n",
    "block_size = 512\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "' Except for Æsthetic Club meetings , the Tower Building remained largely unoccupied for almost fifty years and suffered significant deterioration . The Æsthetic Club provided much @-@ needed financial support during the period and even paid the electric bill during the Great Depression . The Æsthetic Club is still headquartered in the Tower Building . '"
      ]
     },
     "metadata": {},
     "execution_count": 313
    }
   ],
   "source": [
    "raw_datasets['train'][123]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': [18181, 329, 6184, 228, 301, 6587, 6289, 8292, 837, 262, 8765, 11819, 6150, 5688, 555, 28756, 329, 2048, 15334, 812, 290, 6989, 2383, 38495, 764, 383, 6184, 228, 301, 6587, 6289, 2810, 881, 2488, 12, 31, 2622, 3176, 1104, 1141, 262, 2278, 290, 772, 3432, 262, 5186, 2855, 1141, 262, 3878, 22483, 764, 383, 6184, 228, 301, 6587, 6289, 318, 991, 48583, 287, 262, 8765, 11819, 764, 220], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "metadata": {},
     "execution_count": 316
    }
   ],
   "source": [
    "tokenize_function(raw_datasets['train'][123])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=8,\n",
    "    remove_columns=['text'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    num_proc=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['attention_mask', 'input_ids', 'labels'],\n",
       "    num_rows: 228151\n",
       "})"
      ]
     },
     "metadata": {},
     "execution_count": 332
    }
   ],
   "source": [
    "lm_datasets['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Except',\n",
       " 'Ġfor',\n",
       " 'ĠÃ',\n",
       " 'Ĩ',\n",
       " 'st',\n",
       " 'hetic',\n",
       " 'ĠClub',\n",
       " 'Ġmeetings',\n",
       " 'Ġ,',\n",
       " 'Ġthe',\n",
       " 'ĠTower',\n",
       " 'ĠBuilding',\n",
       " 'Ġremained',\n",
       " 'Ġlargely',\n",
       " 'Ġun',\n",
       " 'occupied',\n",
       " 'Ġfor',\n",
       " 'Ġalmost',\n",
       " 'Ġfifty',\n",
       " 'Ġyears',\n",
       " 'Ġand',\n",
       " 'Ġsuffered',\n",
       " 'Ġsignificant',\n",
       " 'Ġdeterioration',\n",
       " 'Ġ.',\n",
       " 'ĠThe',\n",
       " 'ĠÃ',\n",
       " 'Ĩ',\n",
       " 'st',\n",
       " 'hetic',\n",
       " 'ĠClub',\n",
       " 'Ġprovided',\n",
       " 'Ġmuch',\n",
       " 'Ġ@',\n",
       " '-',\n",
       " '@',\n",
       " 'Ġneeded',\n",
       " 'Ġfinancial',\n",
       " 'Ġsupport',\n",
       " 'Ġduring',\n",
       " 'Ġthe',\n",
       " 'Ġperiod',\n",
       " 'Ġand',\n",
       " 'Ġeven',\n",
       " 'Ġpaid',\n",
       " 'Ġthe',\n",
       " 'Ġelectric',\n",
       " 'Ġbill',\n",
       " 'Ġduring',\n",
       " 'Ġthe',\n",
       " 'ĠGreat',\n",
       " 'ĠDepression',\n",
       " 'Ġ.',\n",
       " 'ĠThe',\n",
       " 'ĠÃ',\n",
       " 'Ĩ',\n",
       " 'st',\n",
       " 'hetic',\n",
       " 'ĠClub',\n",
       " 'Ġis',\n",
       " 'Ġstill',\n",
       " 'Ġheadquartered',\n",
       " 'Ġin',\n",
       " 'Ġthe',\n",
       " 'ĠTower',\n",
       " 'ĠBuilding',\n",
       " 'Ġ.']"
      ]
     },
     "metadata": {},
     "execution_count": 334
    }
   ],
   "source": [
    "pretrained_tokenizer.tokenize('Except for Æsthetic Club meetings , the Tower Building remained largely unoccupied for almost fifty years and suffered significant deterioration . The Æsthetic Club provided much @-@ needed financial support during the period and even paid the electric bill during the Great Depression . The Æsthetic Club is still headquartered in the Tower Building .')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}