{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0d1bdbded72f024b9331ead61a2f217ed72f340e06ce2ab6198f65373f5944641",
   "display_name": "Python 3.8.5 64-bit ('anaconda3': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import transformers\n",
    "\n",
    "from rich import print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = True\n",
    "MODEL = \"Helsinki-NLP/opus-mt-en-ro\"\n",
    "SEQ_LENGTH = 32\n",
    "BATCH_SIZE = 8\n",
    "PERCENT = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Reusing dataset wmt16 (/home/tiankang/.cache/huggingface/datasets/wmt16/de-en/1.0.0/0d9fb3e814712c785176ad8cdb9f465fbe6479000ee6546725db30ad8a8b5f8a)\n"
     ]
    }
   ],
   "source": [
    "# dataset = datasets.load_dataset('imdb', split='train[:5%]')  # ['train', 'test', 'unsupervised']\n",
    "dataset = datasets.load_dataset('wmt16', 'de-en')\n",
    "metric = datasets.load_metric('sacrebleu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<module 'datasets' from '/home/tiankang/software/anaconda3/lib/python3.8/site-packages/datasets/__init__.py'>"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001b[1m{\u001b[0m\n    \u001b[32m'translation'\u001b[0m: \u001b[1m{\u001b[0m\n        \u001b[32m'de'\u001b[0m: \u001b[32m'Wiederaufnahme der Sitzungsperiode'\u001b[0m,\n        \u001b[32m'en'\u001b[0m: \u001b[32m'Resumption of the session'\u001b[0m\n    \u001b[1m}\u001b[0m\n\u001b[1m}\u001b[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n    <span style=\"color: #008000; text-decoration-color: #008000\">'translation'</span>: <span style=\"font-weight: bold\">{</span>\n        <span style=\"color: #008000; text-decoration-color: #008000\">'de'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Wiederaufnahme der Sitzungsperiode'</span>,\n        <span style=\"color: #008000; text-decoration-color: #008000\">'en'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Resumption of the session'</span>\n    <span style=\"font-weight: bold\">}</span>\n<span style=\"font-weight: bold\">}</span>\n</pre>\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Metric\u001b[1m(\u001b[0mname: \u001b[32m\"sacrebleu\"\u001b[0m, features: \u001b[1m{\u001b[0m\u001b[32m'predictions'\u001b[0m: Value\u001b[1m(\u001b[0m\u001b[33mdtype\u001b[0m=\u001b[32m'string'\u001b[0m, \u001b[33mid\u001b[0m=\u001b[32m'sequence'\u001b[0m\u001b[1m)\u001b[0m, \n\u001b[32m'references'\u001b[0m: Sequence\u001b[1m(\u001b[0m\u001b[33mfeature\u001b[0m=\u001b[35mValue\u001b[0m\u001b[1m(\u001b[0m\u001b[33mdtype\u001b[0m=\u001b[32m'string'\u001b[0m, \u001b[33mid\u001b[0m=\u001b[32m'sequence'\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mlength\u001b[0m=\u001b[1;36m-1\u001b[0m, \n\u001b[33mid\u001b[0m=\u001b[32m'references'\u001b[0m\u001b[1m)\u001b[0m\u001b[1m}\u001b[0m, usage: \u001b[32m\"\"\u001b[0m\"\nProduces BLEU scores along with its sufficient statistics\nfrom a source against one or more references.\n\nArgs:\n    predictions: The system stream \u001b[1m(\u001b[0ma sequence of segments\u001b[1m)\u001b[0m\n    references: A list of one or more reference streams \u001b[1m(\u001b[0meach a sequence of segments\u001b[1m)\u001b[0m\n    smooth: The smoothing method to use\n    smooth_value: For \u001b[32m'floor'\u001b[0m smoothing, the floor to use\n    force: Ignore data that looks already tokenized\n    lowercase: Lowercase the data\n    tokenize: The tokenizer to use\nReturns:\n    \u001b[32m'score'\u001b[0m: BLEU score,\n    \u001b[32m'counts'\u001b[0m: Counts,\n    \u001b[32m'totals'\u001b[0m: Totals,\n    \u001b[32m'precisions'\u001b[0m: Precisions,\n    \u001b[32m'bp'\u001b[0m: Brevity penalty,\n    \u001b[32m'sys_len'\u001b[0m: predictions length,\n    \u001b[32m'ref_len'\u001b[0m: reference length,\nExamples:\n\n    >>> predictions = \u001b[1m[\u001b[0m\u001b[32m\"hello there general kenobi\"\u001b[0m, \u001b[32m\"foo bar foobar\"\u001b[0m\u001b[1m]\u001b[0m\n    >>> references = \u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[32m\"hello there general kenobi\"\u001b[0m, \u001b[32m\"hello there !\"\u001b[0m\u001b[1m]\u001b[0m, \u001b[1m[\u001b[0m\u001b[32m\"foo bar foobar\"\u001b[0m, \n\u001b[32m\"foo bar foobar\"\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\n    >>> sacrebleu = datasets.load_metric\u001b[1m(\u001b[0m\u001b[32m\"sacrebleu\"\u001b[0m\u001b[1m)\u001b[0m\n    >>> results = sacrebleu.compute\u001b[1m(\u001b[0m\u001b[33mpredictions\u001b[0m=\u001b[35mpredictions\u001b[0m, \u001b[33mreferences\u001b[0m=\u001b[35mreferences\u001b[0m\u001b[1m)\u001b[0m\n    >>> print\u001b[1m(\u001b[0mlist\u001b[1m(\u001b[0mresults.keys\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m\n    \u001b[1m[\u001b[0m\u001b[32m'score'\u001b[0m, \u001b[32m'counts'\u001b[0m, \u001b[32m'totals'\u001b[0m, \u001b[32m'precisions'\u001b[0m, \u001b[32m'bp'\u001b[0m, \u001b[32m'sys_len'\u001b[0m, \u001b[32m'ref_len'\u001b[0m\u001b[1m]\u001b[0m\n    >>> print\u001b[1m(\u001b[0mround\u001b[1m(\u001b[0mresults\u001b[1m[\u001b[0m\u001b[32m\"score\"\u001b[0m\u001b[1m]\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m\n    \u001b[1;36m100.0\u001b[0m\n\u001b[32m\"\"\u001b[0m\", stored examples: \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Metric<span style=\"font-weight: bold\">(</span>name: <span style=\"color: #008000; text-decoration-color: #008000\">\"sacrebleu\"</span>, features: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'predictions'</span>: Value<span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'string'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'sequence'</span><span style=\"font-weight: bold\">)</span>, \n<span style=\"color: #008000; text-decoration-color: #008000\">'references'</span>: Sequence<span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">feature</span>=<span style=\"color: #800080; text-decoration-color: #800080\">Value</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'string'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'sequence'</span><span style=\"font-weight: bold\">)</span>, <span style=\"color: #808000; text-decoration-color: #808000\">length</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1</span>, \n<span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'references'</span><span style=\"font-weight: bold\">)}</span>, usage: <span style=\"color: #008000; text-decoration-color: #008000\">\"\"</span>\"\nProduces BLEU scores along with its sufficient statistics\nfrom a source against one or more references.\n\nArgs:\n    predictions: The system stream <span style=\"font-weight: bold\">(</span>a sequence of segments<span style=\"font-weight: bold\">)</span>\n    references: A list of one or more reference streams <span style=\"font-weight: bold\">(</span>each a sequence of segments<span style=\"font-weight: bold\">)</span>\n    smooth: The smoothing method to use\n    smooth_value: For <span style=\"color: #008000; text-decoration-color: #008000\">'floor'</span> smoothing, the floor to use\n    force: Ignore data that looks already tokenized\n    lowercase: Lowercase the data\n    tokenize: The tokenizer to use\nReturns:\n    <span style=\"color: #008000; text-decoration-color: #008000\">'score'</span>: BLEU score,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'counts'</span>: Counts,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'totals'</span>: Totals,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'precisions'</span>: Precisions,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'bp'</span>: Brevity penalty,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'sys_len'</span>: predictions length,\n    <span style=\"color: #008000; text-decoration-color: #008000\">'ref_len'</span>: reference length,\nExamples:\n\n    &gt;&gt;&gt; predictions = <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">\"hello there general kenobi\"</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"foo bar foobar\"</span><span style=\"font-weight: bold\">]</span>\n    &gt;&gt;&gt; references = <span style=\"font-weight: bold\">[[</span><span style=\"color: #008000; text-decoration-color: #008000\">\"hello there general kenobi\"</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"hello there !\"</span><span style=\"font-weight: bold\">]</span>, <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">\"foo bar foobar\"</span>, \n<span style=\"color: #008000; text-decoration-color: #008000\">\"foo bar foobar\"</span><span style=\"font-weight: bold\">]]</span>\n    &gt;&gt;&gt; sacrebleu = datasets.load_metric<span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"sacrebleu\"</span><span style=\"font-weight: bold\">)</span>\n    &gt;&gt;&gt; results = sacrebleu.compute<span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">predictions</span>=<span style=\"color: #800080; text-decoration-color: #800080\">predictions</span>, <span style=\"color: #808000; text-decoration-color: #808000\">references</span>=<span style=\"color: #800080; text-decoration-color: #800080\">references</span><span style=\"font-weight: bold\">)</span>\n    &gt;&gt;&gt; print<span style=\"font-weight: bold\">(</span>list<span style=\"font-weight: bold\">(</span>results.keys<span style=\"font-weight: bold\">()))</span>\n    <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'score'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'counts'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'totals'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'precisions'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'bp'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'sys_len'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'ref_len'</span><span style=\"font-weight: bold\">]</span>\n    &gt;&gt;&gt; print<span style=\"font-weight: bold\">(</span>round<span style=\"font-weight: bold\">(</span>results<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">\"score\"</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">))</span>\n    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100.0</span>\n<span style=\"color: #008000; text-decoration-color: #008000\">\"\"</span>\", stored examples: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)</span>\n</pre>\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "print(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'score': 100.00000000000004,\n",
       " 'counts': [9, 7, 5, 3],\n",
       " 'totals': [9, 7, 5, 3],\n",
       " 'precisions': [100.0, 100.0, 100.0, 100.0],\n",
       " 'bp': 1.0,\n",
       " 'sys_len': 9,\n",
       " 'ref_len': 9}"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "fake_preds = [\"hello there general kenobi star war\", \"foo bar foobar\"]\n",
    "fake_labels = [[\"hello there general kenobi star war\", \"hello there !\"], [\"foo bar foobar\", \n",
    "\"foo bar foobar\"]]\n",
    "metric.compute(predictions=fake_preds, references=fake_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1132.0, style=ProgressStyle(description…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f1b3aea1270140fc8a72286c36b51912"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=788572.0, style=ProgressStyle(descripti…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "596fcec70828439e933806798c632bcc"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=816966.0, style=ProgressStyle(descripti…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "56c717eea4c04e5ca8a01da3d15b9f87"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1392122.0, style=ProgressStyle(descript…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3f91a2334ef04dd183f9da2e7935358f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=42.0, style=ProgressStyle(description_w…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "655d74db49a64e13aee65fb3b3410da5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL, cache_dir='/home/tiankang/wusuowei/data/huggingface/tokenizers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'mbart' in MODEL:\n",
    "    tokenizer.src_lang = 'en-XX'\n",
    "    tokenizer.tgt_lang = 'ro-RO'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "PreTrainedTokenizer\u001b[1m(\u001b[0m\u001b[33mname_or_path\u001b[0m=\u001b[32m'Helsinki-NLP/opus-mt-en-ro'\u001b[0m, \u001b[33mvocab_size\u001b[0m=\u001b[1;36m59543\u001b[0m, \n\u001b[33mmodel_max_len\u001b[0m=\u001b[1;36m512\u001b[0m, \u001b[33mis_fast\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33mpadding_side\u001b[0m=\u001b[32m'right'\u001b[0m, \u001b[33mspecial_tokens\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'eos_token'\u001b[0m: \u001b[32m'\u001b[0m\u001b[32m<\u001b[0m\u001b[32m/s\u001b[0m\u001b[32m>\u001b[0m\u001b[32m'\u001b[0m, \n\u001b[32m'unk_token'\u001b[0m: \u001b[32m'\u001b[0m\u001b[32m<\u001b[0m\u001b[32munk\u001b[0m\u001b[32m>\u001b[0m\u001b[32m'\u001b[0m, \u001b[32m'pad_token'\u001b[0m: \u001b[32m'\u001b[0m\u001b[32m<\u001b[0m\u001b[32mpad\u001b[0m\u001b[32m>\u001b[0m\u001b[32m'\u001b[0m\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">PreTrainedTokenizer<span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">name_or_path</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Helsinki-NLP/opus-mt-en-ro'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">vocab_size</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">59543</span>, \n<span style=\"color: #808000; text-decoration-color: #808000\">model_max_len</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512</span>, <span style=\"color: #808000; text-decoration-color: #808000\">is_fast</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #808000; text-decoration-color: #808000\">padding_side</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'right'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">special_tokens</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'eos_token'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'&lt;/s&gt;'</span>, \n<span style=\"color: #008000; text-decoration-color: #008000\">'unk_token'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'&lt;unk&gt;'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'pad_token'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'&lt;pad&gt;'</span><span style=\"font-weight: bold\">})</span>\n</pre>\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001b[1m[\u001b[0m\u001b[32m'▁He'\u001b[0m, \u001b[32m'llo'\u001b[0m, \u001b[32m','\u001b[0m, \u001b[32m'▁this'\u001b[0m, \u001b[32m'▁one'\u001b[0m, \u001b[32m'▁sentence'\u001b[0m, \u001b[32m'!'\u001b[0m\u001b[1m]\u001b[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'▁He'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'llo'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">','</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'▁this'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'▁one'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'▁sentence'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'!'</span><span style=\"font-weight: bold\">]</span>\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001b[1m[\u001b[0m\u001b[1;36m125\u001b[0m, \u001b[1;36m778\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m63\u001b[0m, \u001b[1;36m141\u001b[0m, \u001b[1;36m9191\u001b[0m, \u001b[1;36m23\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">125</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">778</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">63</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">141</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9191</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">23</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001b[1m{\u001b[0m\u001b[32m'input_ids'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1;36m125\u001b[0m, \u001b[1;36m778\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m63\u001b[0m, \u001b[1;36m141\u001b[0m, \u001b[1;36m9191\u001b[0m, \u001b[1;36m23\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m, \u001b[32m'attention_mask'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \n\u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'input_ids'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">125</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">778</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">63</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">141</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9191</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">23</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'attention_mask'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, \n<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]}</span>\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001b[1m[\u001b[0m\u001b[32m'▁Hel'\u001b[0m, \u001b[32m'lo'\u001b[0m, \u001b[32m','\u001b[0m, \u001b[32m'▁'\u001b[0m, \u001b[32m'this'\u001b[0m, \u001b[32m'▁o'\u001b[0m, \u001b[32m'ne'\u001b[0m, \u001b[32m'▁se'\u001b[0m, \u001b[32m'nten'\u001b[0m, \u001b[32m'ce'\u001b[0m, \u001b[32m'!'\u001b[0m\u001b[1m]\u001b[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'▁Hel'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'lo'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">','</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'▁'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'this'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'▁o'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'ne'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'▁se'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'nten'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'ce'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'!'</span><span style=\"font-weight: bold\">]</span>\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001b[1m{\u001b[0m\u001b[32m'input_ids'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1;36m10334\u001b[0m, \u001b[1;36m1204\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m15\u001b[0m, \u001b[1;36m8915\u001b[0m, \u001b[1;36m27\u001b[0m, \u001b[1;36m452\u001b[0m, \u001b[1;36m59\u001b[0m, \u001b[1;36m29579\u001b[0m, \u001b[1;36m581\u001b[0m, \u001b[1;36m23\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m, \u001b[32m'attention_mask'\u001b[0m: \n\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'input_ids'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10334</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1204</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8915</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">452</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">59</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">29579</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">581</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">23</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'attention_mask'</span>: \n<span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]}</span>\n</pre>\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "print(tokenizer.tokenize(\"Hello, this one sentence!\"))\n",
    "print(tokenizer.encode(\"Hello, this one sentence!\"))\n",
    "print(tokenizer(\"Hello, this one sentence!\"))\n",
    "\n",
    "with tokenizer.as_target_tokenizer():\n",
    "    # This context manager will make sure the tokenizer uses the special tokens corresponding to the targets.\n",
    "    print(tokenizer.tokenize(\"Hello, this one sentence!\"))\n",
    "    print(tokenizer(\"Hello, this one sentence!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001b[1m{\u001b[0m\u001b[32m'input_ids'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m10334\u001b[0m, \u001b[1;36m1204\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m15\u001b[0m, \u001b[1;36m8915\u001b[0m, \u001b[1;36m27\u001b[0m, \u001b[1;36m452\u001b[0m, \u001b[1;36m59\u001b[0m, \u001b[1;36m29579\u001b[0m, \u001b[1;36m581\u001b[0m, \u001b[1;36m23\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m, \u001b[1m[\u001b[0m\u001b[1;36m235\u001b[0m, \u001b[1;36m1705\u001b[0m, \u001b[1;36m11\u001b[0m, \n\u001b[1;36m32\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[1;36m1205\u001b[0m, \u001b[1;36m5305\u001b[0m, \u001b[1;36m59\u001b[0m, \u001b[1;36m29579\u001b[0m, \u001b[1;36m581\u001b[0m, \u001b[1;36m2\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m, \u001b[32m'attention_mask'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \n\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m, \u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'input_ids'</span>: <span style=\"font-weight: bold\">[[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10334</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1204</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8915</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">452</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">59</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">29579</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">581</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">23</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>, <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">235</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1705</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>, \n<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1205</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5305</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">59</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">29579</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">581</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]]</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'attention_mask'</span>: <span style=\"font-weight: bold\">[[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, \n<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]</span>, <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]]}</span>\n</pre>\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "with tokenizer.as_target_tokenizer():\n",
    "    # This context manager will make sure the tokenizer uses the special tokens corresponding to the targets.\n",
    "    print(tokenizer([\"Hello, this one sentence!\", \"This is another sentence.\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL in [\"t5-small\", \"t5-base\", \"t5-larg\", \"t5-3b\", \"t5-11b\"]:\n",
    "    prefix = \"translate English to Romanian: \"\n",
    "else:\n",
    "    prefix = \"\"\n",
    "\n",
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "source_lang = \"en\"\n",
    "target_lang = \"ro\"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + ex[source_lang] for ex in examples[\"translation\"]]\n",
    "    targets = [ex[target_lang] for ex in examples[\"translation\"]]\n",
    "    # tokenizer for source\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "    # tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001b[1m{\u001b[0m\u001b[32m'input_ids'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m393\u001b[0m, \u001b[1;36m4462\u001b[0m, \u001b[1;36m14\u001b[0m, \u001b[1;36m1137\u001b[0m, \u001b[1;36m53\u001b[0m, \u001b[1;36m216\u001b[0m, \u001b[1;36m28636\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m, \u001b[1m[\u001b[0m\u001b[1;36m24385\u001b[0m, \u001b[1;36m14\u001b[0m, \u001b[1;36m28636\u001b[0m, \u001b[1;36m14\u001b[0m, \u001b[1;36m4646\u001b[0m, \u001b[1;36m4622\u001b[0m, \n\u001b[1;36m53\u001b[0m, \u001b[1;36m216\u001b[0m, \u001b[1;36m28636\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m, \u001b[32m'attention_mask'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m, \u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m,\n\u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m, \u001b[32m'labels'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m42140\u001b[0m, \u001b[1;36m494\u001b[0m, \u001b[1;36m1750\u001b[0m, \u001b[1;36m53\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[1;36m59\u001b[0m, \u001b[1;36m903\u001b[0m, \u001b[1;36m3543\u001b[0m, \u001b[1;36m9\u001b[0m, \u001b[1;36m15202\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m, \u001b[1m[\u001b[0m\u001b[1;36m36199\u001b[0m, \u001b[1;36m6612\u001b[0m, \u001b[1;36m9\u001b[0m, \n\u001b[1;36m15202\u001b[0m, \u001b[1;36m122\u001b[0m, \u001b[1;36m568\u001b[0m, \u001b[1;36m35788\u001b[0m, \u001b[1;36m21549\u001b[0m, \u001b[1;36m53\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[1;36m59\u001b[0m, \u001b[1;36m903\u001b[0m, \u001b[1;36m3543\u001b[0m, \u001b[1;36m9\u001b[0m, \u001b[1;36m15202\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'input_ids'</span>: <span style=\"font-weight: bold\">[[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">393</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4462</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1137</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">53</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">216</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">28636</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>, <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">24385</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">28636</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4646</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4622</span>, \n<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">53</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">216</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">28636</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]]</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'attention_mask'</span>: <span style=\"font-weight: bold\">[[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]</span>, <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]]</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'labels'</span>: <span style=\"font-weight: bold\">[[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">42140</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">494</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1750</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">53</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">59</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">903</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3543</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15202</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>, <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">36199</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6612</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span>, \n<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15202</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">122</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">568</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">35788</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21549</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">53</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">59</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">903</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3543</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15202</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]]}</span>\n</pre>\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "print(preprocess_function(raw_datasets['train'][:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=611.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3913c166bb7949fd8ff8f8a03b2786a9"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3d4ca78aad4447f88cca40a00b75a3c2"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d004fce35c59479aa40f9a423c53bb0c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch_scatter'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-13845f428013>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoModelForSeq2SeqLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataCollatorForSeq2Seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeq2SeqTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeq2SeqTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForSeq2SeqLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/anaconda3/lib/python3.8/site-packages/transformers/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2308\u001b[0m             \u001b[0mTFLongformerForQuestionAnswering\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2309\u001b[0m             \u001b[0mTFLongformerForSequenceClassification\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2310\u001b[0;31m             \u001b[0mTFLongformerForTokenClassification\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2311\u001b[0m             \u001b[0mTFLongformerModel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2312\u001b[0m             \u001b[0mTFLongformerSelfAttention\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/anaconda3/lib/python3.8/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[0mDO_NOT_PAD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"do_not_pad\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1662\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mTensorType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mExplicitEnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m     \"\"\"\n",
      "\u001b[0;32m~/software/anaconda3/lib/python3.8/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1658\u001b[0m     \u001b[0mMAX_LENGTH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"max_length\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[0mDO_NOT_PAD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"do_not_pad\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1660\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1662\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mTensorType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mExplicitEnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/anaconda3/lib/python3.8/site-packages/transformers/models/auto/__init__.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0mTFAutoModelForSeq2SeqLM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0mTFAutoModelForSequenceClassification\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mTFAutoModelForTokenClassification\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m             \u001b[0mTFAutoModelWithLMHead\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         )\n",
      "\u001b[0;32m~/software/anaconda3/lib/python3.8/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/anaconda3/lib/python3.8/site-packages/transformers/models/auto/modeling_auto.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    229\u001b[0m )\n\u001b[1;32m    230\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodeling_t5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mT5ForConditionalGeneration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT5Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m from ..tapas.modeling_tapas import (\n\u001b[0m\u001b[1;32m    232\u001b[0m     \u001b[0mTapasForMaskedLM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0mTapasForQuestionAnswering\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/anaconda3/lib/python3.8/site-packages/transformers/models/tapas/modeling_tapas.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m# soft dependency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_scatter_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_scatter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscatter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_logger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch_scatter'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'label': 1,\n",
       " 'text': 'Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High\\'s satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers\\' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I\\'m here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn\\'t!'}"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.BertTokenizerFast.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[101,\n",
       " 22953,\n",
       " 2213,\n",
       " 4381,\n",
       " 2152,\n",
       " 2003,\n",
       " 1037,\n",
       " 9476,\n",
       " 4038,\n",
       " 1012,\n",
       " 2009,\n",
       " 2743,\n",
       " 2012,\n",
       " 1996,\n",
       " 2168,\n",
       " 2051,\n",
       " 2004,\n",
       " 2070,\n",
       " 2060,\n",
       " 3454,\n",
       " 2055,\n",
       " 2082,\n",
       " 2166,\n",
       " 1010,\n",
       " 2107,\n",
       " 2004,\n",
       " 1000,\n",
       " 5089,\n",
       " 1000,\n",
       " 1012,\n",
       " 2026,\n",
       " 3486,\n",
       " 2086,\n",
       " 1999,\n",
       " 1996,\n",
       " 4252,\n",
       " 9518,\n",
       " 2599,\n",
       " 2033,\n",
       " 2000,\n",
       " 2903,\n",
       " 2008,\n",
       " 22953,\n",
       " 2213,\n",
       " 4381,\n",
       " 2152,\n",
       " 1005,\n",
       " 1055,\n",
       " 18312,\n",
       " 2003,\n",
       " 2172,\n",
       " 3553,\n",
       " 2000,\n",
       " 4507,\n",
       " 2084,\n",
       " 2003,\n",
       " 1000,\n",
       " 5089,\n",
       " 1000,\n",
       " 1012,\n",
       " 1996,\n",
       " 25740,\n",
       " 2000,\n",
       " 5788,\n",
       " 13732,\n",
       " 1010,\n",
       " 1996,\n",
       " 12369,\n",
       " 3993,\n",
       " 2493,\n",
       " 2040,\n",
       " 2064,\n",
       " 2156,\n",
       " 2157,\n",
       " 2083,\n",
       " 2037,\n",
       " 17203,\n",
       " 5089,\n",
       " 1005,\n",
       " 13433,\n",
       " 8737,\n",
       " 1010,\n",
       " 1996,\n",
       " 9004,\n",
       " 10196,\n",
       " 4757,\n",
       " 1997,\n",
       " 1996,\n",
       " 2878,\n",
       " 3663,\n",
       " 1010,\n",
       " 2035,\n",
       " 10825,\n",
       " 2033,\n",
       " 1997,\n",
       " 1996,\n",
       " 2816,\n",
       " 1045,\n",
       " 2354,\n",
       " 1998,\n",
       " 2037,\n",
       " 2493,\n",
       " 1012,\n",
       " 2043,\n",
       " 1045,\n",
       " 2387,\n",
       " 1996,\n",
       " 2792,\n",
       " 1999,\n",
       " 2029,\n",
       " 1037,\n",
       " 3076,\n",
       " 8385,\n",
       " 2699,\n",
       " 2000,\n",
       " 6402,\n",
       " 2091,\n",
       " 1996,\n",
       " 2082,\n",
       " 1010,\n",
       " 1045,\n",
       " 3202,\n",
       " 7383,\n",
       " 1012,\n",
       " 1012,\n",
       " 1012,\n",
       " 1012,\n",
       " 1012,\n",
       " 1012,\n",
       " 1012,\n",
       " 1012,\n",
       " 1012,\n",
       " 2012,\n",
       " 1012,\n",
       " 1012,\n",
       " 1012,\n",
       " 1012,\n",
       " 1012,\n",
       " 1012,\n",
       " 1012,\n",
       " 1012,\n",
       " 1012,\n",
       " 1012,\n",
       " 2152,\n",
       " 1012,\n",
       " 1037,\n",
       " 4438,\n",
       " 2240,\n",
       " 1024,\n",
       " 7742,\n",
       " 1024,\n",
       " 1045,\n",
       " 1005,\n",
       " 1049,\n",
       " 2182,\n",
       " 2000,\n",
       " 12803,\n",
       " 2028,\n",
       " 1997,\n",
       " 2115,\n",
       " 5089,\n",
       " 1012,\n",
       " 3076,\n",
       " 1024,\n",
       " 6160,\n",
       " 2000,\n",
       " 22953,\n",
       " 2213,\n",
       " 4381,\n",
       " 2152,\n",
       " 1012,\n",
       " 1045,\n",
       " 5987,\n",
       " 2008,\n",
       " 2116,\n",
       " 6001,\n",
       " 1997,\n",
       " 2026,\n",
       " 2287,\n",
       " 2228,\n",
       " 2008,\n",
       " 22953,\n",
       " 2213,\n",
       " 4381,\n",
       " 2152,\n",
       " 2003,\n",
       " 2521,\n",
       " 18584,\n",
       " 2098,\n",
       " 1012,\n",
       " 2054,\n",
       " 1037,\n",
       " 12063,\n",
       " 2008,\n",
       " 2009,\n",
       " 3475,\n",
       " 1005,\n",
       " 1056,\n",
       " 999,\n",
       " 102]"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "tokenizer.encode(dataset[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['bro',\n",
       " '##m',\n",
       " '##well',\n",
       " 'high',\n",
       " 'is',\n",
       " 'a',\n",
       " 'cartoon',\n",
       " 'comedy',\n",
       " '.',\n",
       " 'it',\n",
       " 'ran',\n",
       " 'at',\n",
       " 'the',\n",
       " 'same',\n",
       " 'time',\n",
       " 'as',\n",
       " 'some',\n",
       " 'other',\n",
       " 'programs',\n",
       " 'about',\n",
       " 'school',\n",
       " 'life',\n",
       " ',',\n",
       " 'such',\n",
       " 'as',\n",
       " '\"',\n",
       " 'teachers',\n",
       " '\"',\n",
       " '.',\n",
       " 'my',\n",
       " '35',\n",
       " 'years',\n",
       " 'in',\n",
       " 'the',\n",
       " 'teaching',\n",
       " 'profession',\n",
       " 'lead',\n",
       " 'me',\n",
       " 'to',\n",
       " 'believe',\n",
       " 'that',\n",
       " 'bro',\n",
       " '##m',\n",
       " '##well',\n",
       " 'high',\n",
       " \"'\",\n",
       " 's',\n",
       " 'satire',\n",
       " 'is',\n",
       " 'much',\n",
       " 'closer',\n",
       " 'to',\n",
       " 'reality',\n",
       " 'than',\n",
       " 'is',\n",
       " '\"',\n",
       " 'teachers',\n",
       " '\"',\n",
       " '.',\n",
       " 'the',\n",
       " 'scramble',\n",
       " 'to',\n",
       " 'survive',\n",
       " 'financially',\n",
       " ',',\n",
       " 'the',\n",
       " 'insight',\n",
       " '##ful',\n",
       " 'students',\n",
       " 'who',\n",
       " 'can',\n",
       " 'see',\n",
       " 'right',\n",
       " 'through',\n",
       " 'their',\n",
       " 'pathetic',\n",
       " 'teachers',\n",
       " \"'\",\n",
       " 'po',\n",
       " '##mp',\n",
       " ',',\n",
       " 'the',\n",
       " 'pet',\n",
       " '##tine',\n",
       " '##ss',\n",
       " 'of',\n",
       " 'the',\n",
       " 'whole',\n",
       " 'situation',\n",
       " ',',\n",
       " 'all',\n",
       " 'remind',\n",
       " 'me',\n",
       " 'of',\n",
       " 'the',\n",
       " 'schools',\n",
       " 'i',\n",
       " 'knew',\n",
       " 'and',\n",
       " 'their',\n",
       " 'students',\n",
       " '.',\n",
       " 'when',\n",
       " 'i',\n",
       " 'saw',\n",
       " 'the',\n",
       " 'episode',\n",
       " 'in',\n",
       " 'which',\n",
       " 'a',\n",
       " 'student',\n",
       " 'repeatedly',\n",
       " 'tried',\n",
       " 'to',\n",
       " 'burn',\n",
       " 'down',\n",
       " 'the',\n",
       " 'school',\n",
       " ',',\n",
       " 'i',\n",
       " 'immediately',\n",
       " 'recalled',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " 'at',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " 'high',\n",
       " '.',\n",
       " 'a',\n",
       " 'classic',\n",
       " 'line',\n",
       " ':',\n",
       " 'inspector',\n",
       " ':',\n",
       " 'i',\n",
       " \"'\",\n",
       " 'm',\n",
       " 'here',\n",
       " 'to',\n",
       " 'sack',\n",
       " 'one',\n",
       " 'of',\n",
       " 'your',\n",
       " 'teachers',\n",
       " '.',\n",
       " 'student',\n",
       " ':',\n",
       " 'welcome',\n",
       " 'to',\n",
       " 'bro',\n",
       " '##m',\n",
       " '##well',\n",
       " 'high',\n",
       " '.',\n",
       " 'i',\n",
       " 'expect',\n",
       " 'that',\n",
       " 'many',\n",
       " 'adults',\n",
       " 'of',\n",
       " 'my',\n",
       " 'age',\n",
       " 'think',\n",
       " 'that',\n",
       " 'bro',\n",
       " '##m',\n",
       " '##well',\n",
       " 'high',\n",
       " 'is',\n",
       " 'far',\n",
       " 'fetch',\n",
       " '##ed',\n",
       " '.',\n",
       " 'what',\n",
       " 'a',\n",
       " 'pity',\n",
       " 'that',\n",
       " 'it',\n",
       " 'isn',\n",
       " \"'\",\n",
       " 't',\n",
       " '!']"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "tokenizer.tokenize(dataset[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 22953,  2213,  4381,  2152,  2003,  1037,  9476,  4038,  1012,\n",
       "          2009,  2743,  2012,  1996,  2168,  2051,  2004,  2070,  2060,  3454,\n",
       "          2055,  2082,  2166,  1010,  2107,  2004,  1000,  5089,  1000,  1012,\n",
       "          2026,  3486,  2086,  1999,  1996,  4252,  9518,  2599,  2033,  2000,\n",
       "          2903,  2008, 22953,  2213,  4381,  2152,  1005,  1055, 18312,  2003,\n",
       "          2172,  3553,  2000,  4507,  2084,  2003,  1000,  5089,  1000,  1012,\n",
       "          1996, 25740,  2000,  5788, 13732,  1010,  1996, 12369,  3993,  2493,\n",
       "          2040,  2064,  2156,  2157,  2083,  2037, 17203,  5089,  1005, 13433,\n",
       "          8737,  1010,  1996,  9004, 10196,  4757,  1997,  1996,  2878,  3663,\n",
       "          1010,  2035, 10825,  2033,  1997,  1996,  2816,  1045,  2354,  1998,\n",
       "          2037,  2493,  1012,  2043,  1045,  2387,  1996,  2792,  1999,  2029,\n",
       "          1037,  3076,  8385,  2699,  2000,  6402,  2091,  1996,  2082,  1010,\n",
       "          1045,  3202,  7383,  1012,  1012,  1012,  1012,  1012,  1012,  1012,\n",
       "          1012,  1012,  2012,  1012,  1012,  1012,  1012,  1012,  1012,  1012,\n",
       "          1012,  1012,  1012,  2152,  1012,  1037,  4438,  2240,  1024,  7742,\n",
       "          1024,  1045,  1005,  1049,  2182,  2000, 12803,  2028,  1997,  2115,\n",
       "          5089,  1012,  3076,  1024,  6160,  2000, 22953,  2213,  4381,  2152,\n",
       "          1012,  1045,  5987,  2008,  2116,  6001,  1997,  2026,  2287,  2228,\n",
       "          2008, 22953,  2213,  4381,  2152,  2003,  2521, 18584,  2098,  1012,\n",
       "          2054,  1037, 12063,  2008,  2009,  3475,  1005,  1056,   999,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "tokenizer(dataset[0]['text'], return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.BertTokenizerFast.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(x):\n",
    "    x['input_ids'] = tokenizer.encode(\n",
    "        x['text'],\n",
    "        max_length=SEQ_LENGTH,\n",
    "        truncation=True,\n",
    "        # pad_to_max_length=True,\n",
    "        padding='max_length'\n",
    "    )\n",
    "    return x\n",
    "\n",
    "def prepare_dataset(split):\n",
    "    split_pct = BATCH_SIZE if DEBUG else PERCENT\n",
    "    dataset = nlp.load_dataset('imdb', split=f'{split}[:{split_pct}%]')\n",
    "    dataset = dataset.map(tokenize, batched=True)\n",
    "    dataset.set_format(type='torch', columns=['input_ids', 'label'])\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset.map(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': [101,\n",
       "  22953,\n",
       "  2213,\n",
       "  4381,\n",
       "  2152,\n",
       "  2003,\n",
       "  1037,\n",
       "  9476,\n",
       "  4038,\n",
       "  1012,\n",
       "  2009,\n",
       "  2743,\n",
       "  2012,\n",
       "  1996,\n",
       "  2168,\n",
       "  2051,\n",
       "  2004,\n",
       "  2070,\n",
       "  2060,\n",
       "  3454,\n",
       "  2055,\n",
       "  2082,\n",
       "  2166,\n",
       "  1010,\n",
       "  2107,\n",
       "  2004,\n",
       "  1000,\n",
       "  5089,\n",
       "  1000,\n",
       "  1012,\n",
       "  2026,\n",
       "  102],\n",
       " 'label': 1,\n",
       " 'text': 'Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High\\'s satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers\\' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I\\'m here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn\\'t!'}"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Dataset(features: {'text': Value(dtype='string', id=None), 'label': ClassLabel(num_classes=2, names=['neg', 'pos'], names_file=None, id=None)}, num_rows: 1250)"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Dataset(features: {'input_ids': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'label': ClassLabel(num_classes=2, names=['neg', 'pos'], names_file=None, id=None), 'text': Value(dtype='string', id=None)}, num_rows: 1250)"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Dataset(features: {'input_ids': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'label': ClassLabel(num_classes=2, names=['neg', 'pos'], names_file=None, id=None), 'text': Value(dtype='string', id=None)}, num_rows: 1250)"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "train_dataset.set_format(type='torch', columns=['input_ids', 'label'])  #output_all_columns=True)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/tiankang/software/anaconda3/lib/python3.8/site-packages/nlp/utils/py_utils.py:191: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n  return function(data_struct)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': tensor([  101, 22953,  2213,  4381,  2152,  2003,  1037,  9476,  4038,  1012,\n",
       "          2009,  2743,  2012,  1996,  2168,  2051,  2004,  2070,  2060,  3454,\n",
       "          2055,  2082,  2166,  1010,  2107,  2004,  1000,  5089,  1000,  1012,\n",
       "          2026,   102]),\n",
       " 'label': tensor(1)}"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}