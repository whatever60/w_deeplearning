{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0d1bdbded72f024b9331ead61a2f217ed72f340e06ce2ab6198f65373f5944641",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<bound method InteractiveShell.excepthook of <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7fe74a23edf0>>"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from rich import print as rprint\n",
    "from rich.traceback import install\n",
    "install()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import tokenizer as t\n",
    "t = importlib.reload(t) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"./data/\"\n",
    "DATA_DIR_TRAIN = DATA_DIR + \"wikitext-2/wiki.train.tokens\"\n",
    "DATA_DIR_VAL = DATA_DIR + \"wikitext-2/wiki.valid.tokens\"\n",
    "DATA_DIR_TEST = DATA_DIR + \"wikitext-2/wiki.test.tokens\"\n",
    "TOKENIZER_PATH = \"gpt2\"\n",
    "SAVE_DATA_DIR = \"./data/tokenized_gpt.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3084 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "t.tokenize(TOKENIZER_PATH, DATA_DIR_TRAIN, DATA_DIR_VAL, DATA_DIR_TEST, SAVE_DATA_DIR, pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001b[91m╭─\u001b[0m\u001b[91m─────────────────────────── \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[91m ───────────────────────────\u001b[0m\u001b[91m─╮\u001b[0m\n\u001b[91m│\u001b[0m \u001b[33m<ipython-input-351-79758360ff6c>\u001b[0m:\u001b[94m5\u001b[0m in \u001b[92m<module>\u001b[0m                                            \u001b[91m│\u001b[0m\n\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n\u001b[91m│\u001b[0m \u001b[2;33m/home/tiankang/software/anaconda3/lib/python3.8/site-packages/transformers/\u001b[0m\u001b[1;33mtokenization_u\u001b[0m \u001b[91m│\u001b[0m\n\u001b[91m│\u001b[0m \u001b[1;33mtils_base.py\u001b[0m:\u001b[94m2344\u001b[0m in \u001b[92mencode_plus\u001b[0m                                                          \u001b[91m│\u001b[0m\n\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n\u001b[91m│\u001b[0m   \u001b[2m2341 \u001b[0m\u001b[2m│   │   │   \u001b[0m**kwargs,                                                              \u001b[91m│\u001b[0m\n\u001b[91m│\u001b[0m   \u001b[2m2342 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                          \u001b[91m│\u001b[0m\n\u001b[91m│\u001b[0m   \u001b[2m2343 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                           \u001b[91m│\u001b[0m\n\u001b[91m│\u001b[0m \u001b[31m❱ \u001b[0m2344 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._encode_plus(                                                  \u001b[91m│\u001b[0m\n\u001b[91m│\u001b[0m   \u001b[2m2345 \u001b[0m\u001b[2m│   │   │   \u001b[0mtext=text,                                                             \u001b[91m│\u001b[0m\n\u001b[91m│\u001b[0m   \u001b[2m2346 \u001b[0m\u001b[2m│   │   │   \u001b[0mtext_pair=text_pair,                                                   \u001b[91m│\u001b[0m\n\u001b[91m│\u001b[0m   \u001b[2m2347 \u001b[0m\u001b[2m│   │   │   \u001b[0madd_special_tokens=add_special_tokens,                                 \u001b[91m│\u001b[0m\n\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n\u001b[91m│\u001b[0m \u001b[2;33m/home/tiankang/software/anaconda3/lib/python3.8/site-packages/transformers/models/gpt2/\u001b[0m\u001b[1;33mto\u001b[0m \u001b[91m│\u001b[0m\n\u001b[91m│\u001b[0m \u001b[1;33mkenization_gpt2_fast.py\u001b[0m:\u001b[94m173\u001b[0m in \u001b[92m_encode_plus\u001b[0m                                               \u001b[91m│\u001b[0m\n\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n\u001b[91m│\u001b[0m   \u001b[2m170 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mto use it with pretokenized inputs.\u001b[0m\u001b[33m\"\u001b[0m                                   \u001b[91m│\u001b[0m\n\u001b[91m│\u001b[0m   \u001b[2m171 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                           \u001b[91m│\u001b[0m\n\u001b[91m│\u001b[0m   \u001b[2m172 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                            \u001b[91m│\u001b[0m\n\u001b[91m│\u001b[0m \u001b[31m❱ \u001b[0m173 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96msuper\u001b[0m()._encode_plus(*args, **kwargs)                                \u001b[91m│\u001b[0m\n\u001b[91m│\u001b[0m   \u001b[2m174 \u001b[0m\u001b[2m│   \u001b[0m                                                                                \u001b[91m│\u001b[0m\n\u001b[91m│\u001b[0m   \u001b[2m175 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92msave_vocabulary\u001b[0m(\u001b[96mself\u001b[0m, save_directory: \u001b[96mstr\u001b[0m, filename_prefix: Optional[\u001b[96mstr\u001b[0m] = \u001b[91m│\u001b[0m\n\u001b[91m│\u001b[0m   \u001b[2m176 \u001b[0m\u001b[2m│   │   \u001b[0mfiles = \u001b[96mself\u001b[0m._tokenizer.model.save(save_directory, name=filename_prefix)    \u001b[91m│\u001b[0m\n\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n\u001b[91m│\u001b[0m \u001b[2;33m/home/tiankang/software/anaconda3/lib/python3.8/site-packages/transformers/\u001b[0m\u001b[1;33mtokenization_u\u001b[0m \u001b[91m│\u001b[0m\n\u001b[91m│\u001b[0m \u001b[1;33mtils_fast.py\u001b[0m:\u001b[94m458\u001b[0m in \u001b[92m_encode_plus\u001b[0m                                                          \u001b[91m│\u001b[0m\n\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n\u001b[91m│\u001b[0m   \u001b[2m455 \u001b[0m\u001b[2m│   \u001b[0m) -> BatchEncoding:                                                             \u001b[91m│\u001b[0m\n\u001b[91m│\u001b[0m   \u001b[2m456 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                            \u001b[91m│\u001b[0m\n\u001b[91m│\u001b[0m   \u001b[2m457 \u001b[0m\u001b[2m│   │   \u001b[0mbatched_input = [(text, text_pair)] \u001b[94mif\u001b[0m text_pair \u001b[94melse\u001b[0m [text]                \u001b[91m│\u001b[0m\n\u001b[91m│\u001b[0m \u001b[31m❱ \u001b[0m458 \u001b[2m│   │   \u001b[0mbatched_output = \u001b[96mself\u001b[0m._batch_encode_plus(                                   \u001b[91m│\u001b[0m\n\u001b[91m│\u001b[0m   \u001b[2m459 \u001b[0m\u001b[2m│   │   │   \u001b[0mbatched_input,                                                          \u001b[91m│\u001b[0m\n\u001b[91m│\u001b[0m   \u001b[2m460 \u001b[0m\u001b[2m│   │   │   \u001b[0mis_split_into_words=is_split_into_words,                                \u001b[91m│\u001b[0m\n\u001b[91m│\u001b[0m   \u001b[2m461 \u001b[0m\u001b[2m│   │   │   \u001b[0madd_special_tokens=add_special_tokens,                                  \u001b[91m│\u001b[0m\n\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n\u001b[91m│\u001b[0m \u001b[2;33m/home/tiankang/software/anaconda3/lib/python3.8/site-packages/transformers/models/gpt2/\u001b[0m\u001b[1;33mto\u001b[0m \u001b[91m│\u001b[0m\n\u001b[91m│\u001b[0m \u001b[1;33mkenization_gpt2_fast.py\u001b[0m:\u001b[94m163\u001b[0m in \u001b[92m_batch_encode_plus\u001b[0m                                         \u001b[91m│\u001b[0m\n\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n\u001b[91m│\u001b[0m   \u001b[2m160 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mto use it with pretokenized inputs.\u001b[0m\u001b[33m\"\u001b[0m                                   \u001b[91m│\u001b[0m\n\u001b[91m│\u001b[0m   \u001b[2m161 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                           \u001b[91m│\u001b[0m\n\u001b[91m│\u001b[0m   \u001b[2m162 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                            \u001b[91m│\u001b[0m\n\u001b[91m│\u001b[0m \u001b[31m❱ \u001b[0m163 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96msuper\u001b[0m()._batch_encode_plus(*args, **kwargs)                          \u001b[91m│\u001b[0m\n\u001b[91m│\u001b[0m   \u001b[2m164 \u001b[0m\u001b[2m│   \u001b[0m                                                                                \u001b[91m│\u001b[0m\n\u001b[91m│\u001b[0m   \u001b[2m165 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_encode_plus\u001b[0m(\u001b[96mself\u001b[0m, *args, **kwargs) -> BatchEncoding:                       \u001b[91m│\u001b[0m\n\u001b[91m│\u001b[0m   \u001b[2m166 \u001b[0m\u001b[2m│   │   \u001b[0mis_split_into_words = kwargs.get(\u001b[33m\"\u001b[0m\u001b[33mis_split_into_words\u001b[0m\u001b[33m\"\u001b[0m, \u001b[94mFalse\u001b[0m)              \u001b[91m│\u001b[0m\n\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n\u001b[91m│\u001b[0m \u001b[2;33m/home/tiankang/software/anaconda3/lib/python3.8/site-packages/transformers/\u001b[0m\u001b[1;33mtokenization_u\u001b[0m \u001b[91m│\u001b[0m\n\u001b[91m│\u001b[0m \u001b[1;33mtils_fast.py\u001b[0m:\u001b[94m385\u001b[0m in \u001b[92m_batch_encode_plus\u001b[0m                                                    \u001b[91m│\u001b[0m\n\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n\u001b[91m│\u001b[0m   \u001b[2m382 \u001b[0m\u001b[2m│   │   │   \u001b[0mpad_to_multiple_of=pad_to_multiple_of,                                  \u001b[91m│\u001b[0m\n\u001b[91m│\u001b[0m   \u001b[2m383 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                           \u001b[91m│\u001b[0m\n\u001b[91m│\u001b[0m   \u001b[2m384 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                            \u001b[91m│\u001b[0m\n\u001b[91m│\u001b[0m \u001b[31m❱ \u001b[0m385 \u001b[2m│   │   \u001b[0mencodings = \u001b[96mself\u001b[0m._tokenizer.encode_batch(                                   \u001b[91m│\u001b[0m\n\u001b[91m│\u001b[0m   \u001b[2m386 \u001b[0m\u001b[2m│   │   │   \u001b[0mbatch_text_or_text_pairs,                                               \u001b[91m│\u001b[0m\n\u001b[91m│\u001b[0m   \u001b[2m387 \u001b[0m\u001b[2m│   │   │   \u001b[0madd_special_tokens=add_special_tokens,                                  \u001b[91m│\u001b[0m\n\u001b[91m│\u001b[0m   \u001b[2m388 \u001b[0m\u001b[2m│   │   │   \u001b[0mis_pretokenized=is_split_into_words,                                    \u001b[91m│\u001b[0m\n\u001b[91m╰───────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n\u001b[1;91mTypeError: \u001b[0mTextEncodeInput must be Union\u001b[1m[\u001b[0mTextInputSequence, Tuple\u001b[1m[\u001b[0mInputSequence, \nInputSequence\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ff0000; text-decoration-color: #ff0000\">╭──────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #ff0000; text-decoration-color: #ff0000\"> ────────────────────────────╮</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">&lt;ipython-input-351-79758360ff6c&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">5</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/tiankang/software/anaconda3/lib/python3.8/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">tokenization_u</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">tils_base.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2344</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">encode_plus</span>                                                          <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2341 │   │   │   </span>**kwargs,                                                              <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2342 │   │   </span>)                                                                          <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2343 │   │   </span>                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2344 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._encode_plus(                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2345 │   │   │   </span>text=text,                                                             <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2346 │   │   │   </span>text_pair=text_pair,                                                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2347 │   │   │   </span>add_special_tokens=add_special_tokens,                                 <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/tiankang/software/anaconda3/lib/python3.8/site-packages/transformers/models/gpt2/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">to</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">kenization_gpt2_fast.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">173</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_encode_plus</span>                                               <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">170 │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"to use it with pretokenized inputs.\"</span>                                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">171 │   │   </span>)                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">172 │   │   </span>                                                                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>173 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">super</span>()._encode_plus(*args, **kwargs)                                <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">174 │   </span>                                                                                <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">175 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">save_vocabulary</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, save_directory: <span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>, filename_prefix: Optional[<span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>] = <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176 │   │   </span>files = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._tokenizer.model.save(save_directory, name=filename_prefix)    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/tiankang/software/anaconda3/lib/python3.8/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">tokenization_u</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">tils_fast.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">458</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_encode_plus</span>                                                          <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">455 │   </span>) -&gt; BatchEncoding:                                                             <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">456 │   │   </span>                                                                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">457 │   │   </span>batched_input = [(text, text_pair)] <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> text_pair <span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> [text]                <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>458 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>batched_output = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._batch_encode_plus(                                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">459 │   │   │   </span>batched_input,                                                          <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">460 │   │   │   </span>is_split_into_words=is_split_into_words,                                <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">461 │   │   │   </span>add_special_tokens=add_special_tokens,                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/tiankang/software/anaconda3/lib/python3.8/site-packages/transformers/models/gpt2/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">to</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">kenization_gpt2_fast.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">163</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_batch_encode_plus</span>                                         <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">160 │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"to use it with pretokenized inputs.\"</span>                                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">161 │   │   </span>)                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">162 │   │   </span>                                                                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>163 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">super</span>()._batch_encode_plus(*args, **kwargs)                          <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">164 │   </span>                                                                                <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">165 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_encode_plus</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, *args, **kwargs) -&gt; BatchEncoding:                       <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">166 │   │   </span>is_split_into_words = kwargs.get(<span style=\"color: #808000; text-decoration-color: #808000\">\"is_split_into_words\"</span>, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">False</span>)              <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/tiankang/software/anaconda3/lib/python3.8/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">tokenization_u</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">tils_fast.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">385</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_batch_encode_plus</span>                                                    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">382 │   │   │   </span>pad_to_multiple_of=pad_to_multiple_of,                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">383 │   │   </span>)                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">384 │   │   </span>                                                                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>385 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>encodings = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._tokenizer.encode_batch(                                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">386 │   │   │   </span>batch_text_or_text_pairs,                                               <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">387 │   │   │   </span>add_special_tokens=add_special_tokens,                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">388 │   │   │   </span>is_pretokenized=is_split_into_words,                                    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000\">╰───────────────────────────────────────────────────────────────────────────────────────────╯</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">TypeError: </span>TextEncodeInput must be Union<span style=\"font-weight: bold\">[</span>TextInputSequence, Tuple<span style=\"font-weight: bold\">[</span>InputSequence, \nInputSequence<span style=\"font-weight: bold\">]]</span>\n</pre>\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "DATA_DIR = './data/'\n",
    "DATA_DIR_TRAIN = DATA_DIR + 'wikitext-2/wiki.train.tokens'\n",
    "with open(DATA_DIR_TRAIN) as f:\n",
    "    entries_train = f.read().split(\" \\n \\n = \")\n",
    "a = tokenizer(entries_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = ['The name of the president of the United States is']\n",
    "encoded_input = tokenizer(test_sample, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 464, 1438,  286,  262, 1893,  286,  262, 1578, 1829,  318]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "metadata": {},
     "execution_count": 319
    }
   ],
   "source": [
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "metadata": {},
     "execution_count": 288
    }
   ],
   "source": [
    "encoded_input.input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['__annotations__',\n",
       " '__class__',\n",
       " '__contains__',\n",
       " '__dataclass_fields__',\n",
       " '__dataclass_params__',\n",
       " '__delattr__',\n",
       " '__delitem__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__post_init__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__reversed__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " 'attentions',\n",
       " 'clear',\n",
       " 'copy',\n",
       " 'cross_attentions',\n",
       " 'fromkeys',\n",
       " 'get',\n",
       " 'hidden_states',\n",
       " 'items',\n",
       " 'keys',\n",
       " 'logits',\n",
       " 'loss',\n",
       " 'move_to_end',\n",
       " 'past_key_values',\n",
       " 'pop',\n",
       " 'popitem',\n",
       " 'setdefault',\n",
       " 'to_tuple',\n",
       " 'update',\n",
       " 'values']"
      ]
     },
     "metadata": {},
     "execution_count": 290
    }
   ],
   "source": [
    "dir(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 50257])"
      ]
     },
     "metadata": {},
     "execution_count": 291
    }
   ],
   "source": [
    "output.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(12, 2, torch.Size([1, 12, 10, 64]), torch.Size([1, 12, 10, 64]))"
      ]
     },
     "metadata": {},
     "execution_count": 292
    }
   ],
   "source": [
    "len(output.past_key_values), len(output.past_key_values[0]), output.past_key_values[0][1].shape, output.past_key_values[0][0].shape  # [batch_size, num_heads, seq_length, emb_size_per_head]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "' not'"
      ]
     },
     "metadata": {},
     "execution_count": 293
    }
   ],
   "source": [
    "tokenizer.decode(output.logits[:, -1, :].argmax(dim=1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(torch.Size([1, 12, 8, 64]), torch.Size([1, 12, 8, 64]))"
      ]
     },
     "metadata": {},
     "execution_count": 257
    }
   ],
   "source": [
    "output.past_key_values[0][0].shape, output.past_key_values[0][1].shape, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datamodule import WikiText2DataModule\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './data/tokenized_gpt.pkl'\n",
    "BATCH_SIZE = 128\n",
    "BPTT_TRAIN = 64\n",
    "BPTT_VAL = 64\n",
    "datamodule = WikiText2DataModule(DATA_DIR, BATCH_SIZE, BPTT_TRAIN, BPTT_VAL)\n",
    "datamodule.setup(stage='fit')\n",
    "loader = datamodule.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:9'\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "model.eval()\n",
    "losses = []\n",
    "seq_lengths = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=32.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aa14aed76e11499890bfb299434767dc"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for batch in tqdm(loader):\n",
    "        input_idxs, target_idxs = batch\n",
    "        # rprint(target_idxs.shape)\n",
    "        input_idxs, target_idxs = input_idxs.to(device), target_idxs.to(device)  # both: [seq_length, batch_size]\n",
    "        logits = model(input_idxs).logits  # [seq_length, batch_size, vocab_size]\n",
    "        seq_length = input_idxs.shape[0]\n",
    "        loss = criterion(logits.flatten(end_dim=1), target_idxs.flatten())\n",
    "        losses.append(loss)\n",
    "        seq_lengths.append(seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(9649.8438, device='cuda:9')"
      ]
     },
     "metadata": {},
     "execution_count": 314
    }
   ],
   "source": [
    "total_loss = sum([loss * seq_length for loss, seq_length in zip(losses, seq_lengths)])\n",
    "total_len = sum(seq_lengths)\n",
    "torch.exp(total_loss / total_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}